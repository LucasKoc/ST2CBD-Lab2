{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Spark Context",
   "id": "ba709b154847d136"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:12:42.016541Z",
     "start_time": "2025-05-15T17:12:41.933621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pip install pyspark\n",
    "from pyspark import SparkContext, RDD\n",
    "\n",
    "DIR_PATH = \"Sparks_dataset/Sparks_dataset\"\n",
    "DIR_CONTEXT = DIR_PATH + \"/Context\"\n",
    "DIR_DATA_FRAME = DIR_PATH + \"/Data Frame\"\n",
    "DIR_SPARK_PANDAS = DIR_PATH + \"/Spark Pandas\""
   ],
   "id": "95868fa35c1cb0a2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 1 - Basic Order Analysis\n",
    "Tasks:\n",
    "\n",
    "1. Load the dataset into Spark using SparkContext and textFile().\n",
    "2. Count the total number of orders.\n",
    "3. Calculate the total revenue from all orders.\n",
    "4. Find the number of unique customers."
   ],
   "id": "e67fddf6f9076e31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:12:43.974853Z",
     "start_time": "2025-05-15T17:12:42.103651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load the dataset into Spark using SparkContext and textFile().\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-OrderAnalysis\")\n",
    "\n",
    "# Load the dataset\n",
    "salesdata_rdd = sc.textFile(f\"{DIR_CONTEXT}/salesdata.csv.txt\")"
   ],
   "id": "c7cc47fcb82bc656",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/15 19:12:42 WARN Utils: Your hostname, MacBook-Pro-de-Lucas.local resolves to a loopback address: 127.0.0.1; using 10.0.0.18 instead (on interface en0)\n",
      "25/05/15 19:12:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/15 19:12:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:12:44.730625Z",
     "start_time": "2025-05-15T17:12:43.986827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Count the total number of orders.\n",
    "# Because the file has 2 header lines, we need to remove them before counting\n",
    "header = salesdata_rdd.take(2)\n",
    "\n",
    "# We remove the header lines from the RDD\n",
    "salesdata_rdd = salesdata_rdd.filter(\n",
    "    lambda line: line != header[0] and line != header[1]\n",
    ")\n",
    "\n",
    "# Count the total number of orders\n",
    "total_orders = salesdata_rdd.count()\n",
    "\n",
    "total_orders"
   ],
   "id": "86c575dae4b985c5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the CSV file, we have 2 header lines. In order to count the total number of orders, we need to remove the header lines from the RDD.\n",
    "\n",
    "Total number of orders: 10"
   ],
   "id": "1a508f782e74823"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:12:44.769107Z",
     "start_time": "2025-05-15T17:12:44.741990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Calculate the total revenue from all orders.\n",
    "# Index of OrderAmount is 2\n",
    "\n",
    "sales_revenues = salesdata_rdd.map(lambda line: float(line.split(\",\")[2]))\n",
    "\n",
    "total_revenue = sales_revenues.sum()\n",
    "\n",
    "total_orders"
   ],
   "id": "b34c4792434cd3ec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Total orders = 10",
   "id": "6d246ad2ccd2ef50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:12:45.264094Z",
     "start_time": "2025-05-15T17:12:45.044534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Find the number of unique customers.\n",
    "# Index of CustomerID is 1\n",
    "\n",
    "unique_customers = salesdata_rdd.map(lambda line: line.split(\",\")[1]).distinct()\n",
    "\n",
    "total_unique_customers = unique_customers.count()\n",
    "\n",
    "total_unique_customers"
   ],
   "id": "8fe76ddec7eeefd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Total unique customers = 6",
   "id": "74470cb99553048e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:12:46.261016Z",
     "start_time": "2025-05-15T17:12:45.292981Z"
    }
   },
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "99fa81529213e507",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 2 - Department Salary Stats\n",
    "Tasks:\n",
    "\n",
    "1. Load employee data and skip header.\n",
    "2. Find total number of employees.\n",
    "3. Calculate average salary per department.\n",
    "4. Find department with highest total salary."
   ],
   "id": "f01eebc57846386b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:12:46.901666Z",
     "start_time": "2025-05-15T17:12:46.268731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load employee data and skip header.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-DepartmentSalaryStats\")\n",
    "\n",
    "# Load the dataset\n",
    "salesdata_rdd = sc.textFile(f\"{DIR_CONTEXT}/2 salary.csv\")\n",
    "\n",
    "# Skip header\n",
    "header = salesdata_rdd.take(1)\n",
    "\n",
    "salesdata_rdd = salesdata_rdd.filter(lambda line: line != header[0])\n",
    "\n",
    "salesdata_rdd.take(1)"
   ],
   "id": "f74f352ed5802b88",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E101,Alice,Sales,50000']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Result should not be the header and be the first line: ['E101,Alice,Sales,50000']",
   "id": "a550e53efe49c99b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:12:46.988057Z",
     "start_time": "2025-05-15T17:12:46.928890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Find total number of employees.\n",
    "\n",
    "total_employees = salesdata_rdd.count()\n",
    "\n",
    "total_employees"
   ],
   "id": "227aa7a1024b4fd5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Total employees = 8",
   "id": "4edc37aafcd4a52c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:12:47.065391Z",
     "start_time": "2025-05-15T17:12:47.001882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Calculate average salary per department.\n",
    "\n",
    "average_salary_per_department = (\n",
    "    salesdata_rdd.map(lambda line: (line.split(\",\")[2], float(line.split(\",\")[3])))\n",
    "    .groupByKey()\n",
    "    .mapValues(lambda salaries: sum(salaries) / len(salaries))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "average_salary_per_department"
   ],
   "id": "cc436f0fea17412f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sales', 54000.0), ('Marketing', 56500.0), ('IT', 69000.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Departements average salaries:\n",
    "- Sales: 54000\n",
    "- Marketing: 56500\n",
    "- IT: 69000"
   ],
   "id": "9725d09411ebae4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:12:47.121980Z",
     "start_time": "2025-05-15T17:12:47.075604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Find department with the highest total salary.\n",
    "\n",
    "highest_salary_department = (\n",
    "    salesdata_rdd.map(lambda line: (line.split(\",\")[2], float(line.split(\",\")[3])))\n",
    "    .groupByKey()\n",
    "    .mapValues(lambda salaries: sum(salaries))\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "    .first()\n",
    ")\n",
    "\n",
    "highest_salary_department"
   ],
   "id": "44615ce9aac8dfe1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('IT', 207000.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-15T17:12:47.175068Z"
    }
   },
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "55d1a074e45e93f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 3: Web Traffic Summary\n",
    "Dataset: web_logs.txt\n",
    "\n",
    "Tasks:\n",
    "1. Load the data and parse it into fields.\n",
    "2. Count total number of visits per URL.\n",
    "3. Find unique users per URL.\n",
    "4. Determine peak hour for traffic."
   ],
   "id": "6edab637bed4684e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load the data and parse it into fields.\n",
    "\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-WebTrafficSummary\")\n",
    "web_logs_rdd = sc.textFile(f\"{DIR_CONTEXT}/3 web_logs.txt\")\n",
    "\n",
    "# Skip header\n",
    "header = web_logs_rdd.take(1)\n",
    "web_logs_rdd = web_logs_rdd.filter(lambda line: line != header[0])\n",
    "\n",
    "# Parse the data into fields\n",
    "web_logs_rdd = web_logs_rdd.map(lambda line: line.split(\",\"))"
   ],
   "id": "785b94fe85957533",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Count total number of visits per URL.\n",
    "\n",
    "url_visits = (\n",
    "    web_logs_rdd.map(lambda fields: (fields[1], 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "url_visits"
   ],
   "id": "c4677f721b4c86f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We take the URL and the number of visits -> Reduce by key to count visits (line = 1 visit).\n",
    "\n",
    "Result: [('/home', 6), ('/product', 2), ('/contact', 1), ('/about', 1)]"
   ],
   "id": "63f7a4dff22daf7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Find unique users per URL.\n",
    "\n",
    "unique_users_per_url = (\n",
    "    web_logs_rdd.map(lambda fields: (fields[1], fields[0]))\n",
    "    .distinct()\n",
    "    .map(lambda x: (x[0], 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "unique_users_per_url"
   ],
   "id": "3632b9b2735e74d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First we take the URL and the user ID -> Remove duplicates -> Reduce by key to count unique users.\n",
    "\n",
    "Result: [('/home', 4), ('/product', 2), ('/contact', 1), ('/about', 1)]"
   ],
   "id": "a64a55cd22098268"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Determine peak hour for traffic.\n",
    "from datetime import datetime\n",
    "\n",
    "hour_counts = web_logs_rdd.map(\n",
    "    lambda fields: (datetime.strptime(fields[2], \"%Y-%m-%d %H:%M:%S\").hour, 1)\n",
    ").reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "peak_hour = hour_counts.sortBy(lambda x: x[1], ascending=False).first()\n",
    "\n",
    "peak_hour"
   ],
   "id": "45aa309e2f180eec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We take the field index 2 (Timestamp) and convert it to a datetime object -> Get the hour -> Reduce by key to count visits per hour.\n",
    "\n",
    "After that we sort by the number of visits and take the first one.\n",
    "\n",
    "Result: (13, 2) -> 13:00 w/ 2 visits"
   ],
   "id": "583ec38036cfe7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "329c7e87d5d781df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 4: Product Rating Analysis\n",
    "Tasks: Dataset: product_reviews.csv\n",
    "1. Load and parse the review data.\n",
    "2. Calculate the average rating per product.\n",
    "3. Count number of reviews per product.\n",
    "4. Find products with all 5-star reviews."
   ],
   "id": "d4e368e4eb5e0ebf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_rdd_csv(file_path, header_lines=1) -> RDD[str]:\n",
    "    rdd = sc.textFile(file_path)\n",
    "    header = rdd.take(header_lines)\n",
    "    rdd = rdd.filter(lambda line: line not in header)\n",
    "\n",
    "    rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "    return rdd"
   ],
   "id": "a1da1dc25c43d723",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Created a function for parsing input files. It takes the file path and the number of header lines to skip (header). It returns an RDD with the parsed data.",
   "id": "73ed62f052e0348e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load and parse the review data.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-ProductRatingAnalysis\")\n",
    "production_reviews_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/4 product_reviews.csv\")"
   ],
   "id": "7306bfedeec312bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Calculate the average rating per product\n",
    "average_rating_per_product = (\n",
    "    production_reviews_rdd.map(lambda fields: (fields[1], float(fields[2])))\n",
    "    .groupByKey()\n",
    "    .mapValues(lambda ratings: sum(ratings) / len(ratings))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "average_rating_per_product"
   ],
   "id": "e7ca0319ccb11219",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second (product) and third field (score) -> Group by product -> Calculate the average score.",
   "id": "fae3f68d8b027e7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Count number of reviews per product\n",
    "\n",
    "reviews_per_product = (\n",
    "    production_reviews_rdd.map(lambda fields: (fields[1], 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "reviews_per_product"
   ],
   "id": "f97835dfd8a0e184",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second field (product) and count the number of reviews (each line correspond to one product review).",
   "id": "df558353ba2c972d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Find products with all 5-star reviews.\n",
    "\n",
    "five_star_products = (\n",
    "    production_reviews_rdd.map(lambda fields: (fields[1], float(fields[2])))\n",
    "    .groupByKey()\n",
    "    .filter(lambda x: all(rating == 5.0 for rating in x[1]))\n",
    "    .map(lambda x: x[0])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "five_star_products"
   ],
   "id": "8117518fa47e9efd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second field (product) and the third field (score) -> Group by product -> Filter products with all 5-star reviews",
   "id": "ed1767d18fb97e8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "4a13d5a10703d42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 5: Movie Ratings Analytics\n",
    "Objective: Learn to read a CSV file, initialize SparkContext, and perform basic operations.\n",
    "\n",
    "Tasks:\n",
    "1. Load the movies.csv using SparkContext.textFile.\n",
    "2. Count the total number of movies.\n",
    "3. Filter movies with rating >= 4.\n",
    "4. Display the top 5 movies by rating."
   ],
   "id": "347df52a5618099e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load the movies.csv using SparkContext.textFile.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-MovieRatingsAnalytics\")\n",
    "movies_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/5 movies.csv\")"
   ],
   "id": "5db0da88a7a1a01c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Count the total number of movies.\n",
    "total_movies = movies_rdd.count()\n",
    "total_movies"
   ],
   "id": "8a79ba93e525e108",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1 line = 1 movie\n",
    "\n",
    "Total movies = 5"
   ],
   "id": "e2c572ad739a7634"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Filter movies with rating >= 4.\n",
    "movies_rated_4_or_plus = (\n",
    "    movies_rdd.filter(lambda fields: float(fields[3]) >= 4.0)\n",
    "    .map(lambda fields: (fields[1], fields[2], float(fields[3])))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "movies_rated_4_or_plus"
   ],
   "id": "3b2ac5644bc63eec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filter lines with rating >= 4.0 -> We take the second field (movie name), third field (genre) and fourth field (rating).",
   "id": "8a2b53aa4f2f480"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Display the top 5 movies by rating. (Sort + Limit)\n",
    "top_5_movies = (\n",
    "    movies_rdd.map(lambda fields: (fields[1], fields[2], float(fields[3])))\n",
    "    .sortBy(lambda fields: float(fields[2]), ascending=False)\n",
    "    .take(5)\n",
    ")\n",
    "\n",
    "top_5_movies"
   ],
   "id": "1870513ca63daab9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second field (movie name), third field (genre) and fourth field (rating) -> Sort by rating -> Take the top 5 movies.",
   "id": "d06422d9e4268e31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "afb2da8fb1627dfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 6: Student Scores Analysis\n",
    "Objective: Understand RDD transformations and actions.\n",
    "\n",
    "Tasks:\n",
    "1. Load data using SparkContext.textFile.\n",
    "2. Map each line to a key-value pair (name, score).\n",
    "3. Filter students who scored above 80.\n",
    "4. Count how many students scored above 80."
   ],
   "id": "d20b266fa057123b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load data using SparkContext.textFile.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-StudentScoresAnalysis\")\n",
    "students_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/6 analysis.csv\")\n",
    "\n",
    "students_rdd.collect()"
   ],
   "id": "5e3fae8389804ee5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Map each line to a key-value pair (name, score).\n",
    "students_scores_rdd = students_rdd.map(lambda fields: (fields[0], float(fields[2])))\n",
    "\n",
    "students_scores_rdd.collect()"
   ],
   "id": "1f779943adfb4ea5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the first field (name) and the third field (score) -> We create a tuple with the name and score.",
   "id": "4d557a3d51767bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Filter students who scored above 80.\n",
    "students_above_80 = (\n",
    "    students_rdd.filter(lambda fields: float(fields[2]) > 80.0)\n",
    "    .map(lambda fields: (fields[0], fields[1]))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "students_above_80"
   ],
   "id": "bd25a50e0872b9cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filter by score > 80.0 (field n°3) -> We take the first field (name) and the second field (subject).",
   "id": "8c3290a5ec0de093"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Count how many students scored above 80.\n",
    "count_students_above_80 = students_rdd.filter(\n",
    "    lambda fields: float(fields[2]) > 80.0\n",
    ").count()\n",
    "\n",
    "print(count_students_above_80)\n",
    "\n",
    "# or\n",
    "count_students_above_80 = len(students_above_80)\n",
    "count_students_above_80"
   ],
   "id": "fb7d5494743023f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "1e782280f7324144",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 7: Word Count from News Articles\n",
    "Objective: Classic Word Count example.\n",
    "\n",
    "Tasks:\n",
    "1. Load the news data file.\n",
    "2. Split lines into words.\n",
    "3. Map each word to (word, 1).\n",
    "4. Use reduceByKey to get word counts"
   ],
   "id": "ae645543e8ce0667"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load the news data file.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-WordCount\")\n",
    "news_rdd = sc.textFile(f\"{DIR_CONTEXT}/7 wordcount.csv\", 0)\n",
    "\n",
    "news_rdd.collect()"
   ],
   "id": "31f990da0329dc56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Split lines into words.\n",
    "words_rdd = news_rdd.flatMap(lambda line: line.replace(\".\", \"\").split(\" \"))\n",
    "\n",
    "words_rdd.collect()"
   ],
   "id": "3c5d3fba22c6dbd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split words by space",
   "id": "61018b29ac4cb3ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Map each word to (word, 1).\n",
    "word_pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "word_pairs_rdd.collect()"
   ],
   "id": "c319efc7747662f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take each word and create a tuple with the word and 1 (1 occurrence).",
   "id": "558bf4911350cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Use reduceByKey to get word counts\n",
    "word_counts_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "word_counts_rdd.collect()"
   ],
   "id": "fa1ffc9af32ecaa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "f2826fd70b101c16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 8: Product Sales Tracker\n",
    "Objective: Group and aggregate product sales.\n",
    "\n",
    "Tasks:\n",
    "1. Load product sales CSV.\n",
    "2. Map to (product, revenue).\n",
    "3. Use reduceByKey to get total revenue.\n",
    "4. Filter products with revenue > 1000."
   ],
   "id": "b3554ac1f38db64b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load product sales CSV.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-ProductSalesTracker\")\n",
    "product_sales_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/8 product.csv\")\n",
    "\n",
    "product_sales_rdd.collect()"
   ],
   "id": "e2ef78c928d8c073",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Map to (product, revenue).\n",
    "product_revenue_rdd = product_sales_rdd.map(\n",
    "    lambda fields: (fields[1], float(fields[2]) * float(fields[3]))\n",
    ")\n",
    "\n",
    "product_revenue_rdd.collect()"
   ],
   "id": "fb5df9acd52fd7f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second field (product) and the third field (revenue) -> We create a tuple with the product and revenue.",
   "id": "106c2da76ba22d6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Use reduceByKey to get total revenue.\n",
    "total_revenue_rdd = product_revenue_rdd.reduceByKey(lambda a, b: a + b)  # ??\n",
    "\n",
    "total_revenue_rdd.collect()\n",
    "\n",
    "# Getwhole total revenue\n",
    "total_revenue = total_revenue_rdd.map(lambda fields: fields[1]).sum()\n",
    "\n",
    "total_revenue"
   ],
   "id": "d8f3e55add70aeae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this case, the usage of reduceByKey is not necessary since there's no duplicate keys.\n",
    "\n",
    "To get the total revenue, we can simply sum values."
   ],
   "id": "6d856487fd289e41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Filter products with revenue > 1000.\n",
    "filtered_revenue_rdd = total_revenue_rdd.filter(lambda fields: fields[1] > 1000)\n",
    "\n",
    "filtered_revenue_rdd.collect()"
   ],
   "id": "c6b0796c461b0b0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filter the products with revenue > 1000.",
   "id": "78055f9e7091357d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "8ce4955c21814355",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 9: Temperature Data Monitoring\n",
    "Objective: Work with sensor data and aggregate.\n",
    "\n",
    "Tasks:\n",
    "1. Load temperature log.\n",
    "2. Extract date and temperature.\n",
    "3. Calculate daily average temperature.\n",
    "4. Filter days with average temperature > 30°C."
   ],
   "id": "adad2448fcf7e44d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load temperature log.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-TemperatureDataMonitoring\")\n",
    "temperature_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/9 temp.csv\")\n",
    "\n",
    "temperature_rdd.collect()"
   ],
   "id": "609577da92635523",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Extract date and temperature.\n",
    "temperature_data_rdd = temperature_rdd.map(lambda fields: (fields[0], float(fields[2])))\n",
    "\n",
    "temperature_data_rdd.collect()"
   ],
   "id": "54c9d7616980ff6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the first field (date) and the third field (temperature) -> We create a tuple with the date and temperature.",
   "id": "8b7cff052ec82186"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Calculate daily average temperature.\n",
    "daily_avg_temp_rdd = temperature_data_rdd.groupByKey().mapValues(\n",
    "    lambda temps: sum(temps) / len(temps)\n",
    ")\n",
    "\n",
    "daily_avg_temp_rdd.collect()"
   ],
   "id": "269171983cc220c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We use groupByKey to group by date -> We calculate the average temperature for each date.",
   "id": "4717fffb8453e38e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Filter days with average temperature > 30°C.\n",
    "filtered_temp_rdd = daily_avg_temp_rdd.filter(lambda fields: fields[1] > 30)\n",
    "\n",
    "filtered_temp_rdd.collect()"
   ],
   "id": "4a73e6b9b96e1321",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the average temperature (produced previously) and filter the days with average temperature > 30°C.",
   "id": "f18af86ccf1e44ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "bf2bea5b4c86e465",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 10: Customer Feedback Sentiment\n",
    "Objective: Perform simple NLP with Spark.\n",
    "\n",
    "Tasks:\n",
    "1. Load feedback text file.\n",
    "2. Clean the text (lowercase, remove punctuation).\n",
    "3. Tokenize and count positive/negative words.\n",
    "4. Classify as positive/negative based on score."
   ],
   "id": "f615f656090faa40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load feedback text file.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-CustomerFeedbackSentiment\")\n",
    "feedback_rdd = sc.textFile(f\"{DIR_CONTEXT}/10 feedback.csv\")\n",
    "\n",
    "feedback_rdd.collect()"
   ],
   "id": "c7824aff9abe30ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Clean the text (lowercase, remove punctuation).\n",
    "\n",
    "clean_text = feedback_rdd.map(lambda line: line.lower().replace(\".\", \"\"))\n",
    "\n",
    "clean_text.collect()"
   ],
   "id": "f4c21d44ff1f930a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here, punctuation is only composed of dots. We simply use lower() function to lowercase text and replace dots with empty string.\n",
    "\n",
    "Result: ['the service was excellent and food was amazing',\n",
    " 'terrible experience the wait time was long',\n",
    " 'loved the ambiance and friendly staff']"
   ],
   "id": "4753a9ac54d3a71a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Tokenize and count positive/negative words.\n",
    "positive_words = [\"excellent\", \"amazing\", \"loved\", \"friendly\"]\n",
    "negative_words = [\"terrible\", \"long\"]\n",
    "\n",
    "\n",
    "def count_sentiment_words(line) -> (int, int):\n",
    "    words = line.split()\n",
    "    positive_count = len([1 for word in words if word in positive_words])\n",
    "    negative_count = len([1 for word in words if word in negative_words])\n",
    "    return positive_count, negative_count\n",
    "\n",
    "\n",
    "sentiment_counts = clean_text.map(count_sentiment_words)\n",
    "\n",
    "sentiment_counts.collect()"
   ],
   "id": "3b55faf473077dce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Asked to tokenize to count positive and negative words. <br>\n",
    "We split the line into words -> We count the number of positive and negative words (defined above). <br>\n",
    "We map the clean text to the count_sentiment_words function.\n",
    "\n",
    "Result:\n",
    "- 1st line: 2 positive, 0 negative\n",
    "- 2nd line: 0 positive, 2 negative\n",
    "- 3rd line: 2 positive, 0 negative"
   ],
   "id": "ab641e9a2854e7da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Classify as positive/negative based on score.\n",
    "\n",
    "\n",
    "def classify_sentiment(counts) -> str:\n",
    "    positive_count, negative_count = counts\n",
    "    if positive_count > negative_count:\n",
    "        return \"Positive\"\n",
    "    elif positive_count < negative_count:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "\n",
    "sentiment_classification = sentiment_counts.map(classify_sentiment)\n",
    "\n",
    "sentiment_classification.collect()"
   ],
   "id": "6411a249ba7aa733",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We fetch the positive and negative counts -> We classify as positive, negative or neutral based on the counts.\n",
    "\n",
    "Result:\n",
    "- 1st line: Positive\n",
    "- 2nd line: Negative\n",
    "- 3rd line: Positive"
   ],
   "id": "96536da1aed5dbd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "6bc606285145a90e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 11: Email Logs Analyzer\n",
    "Objective: Analyze organizational email traffic.\n",
    "\n",
    "Tasks:\n",
    "1. Load logs.\n",
    "2. Map each line to (sender, 1).\n",
    "3. Count emails sent by each user.\n",
    "4. Identify the top 3 senders"
   ],
   "id": "994d801fc1a13d89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load logs.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-EmailLogsAnalyzer\")\n",
    "email_logs_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/11 email.csv\")\n",
    "\n",
    "email_logs_rdd.collect()"
   ],
   "id": "82cad87ee154bbdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Map each line to (sender, 1).\n",
    "email_senders_rdd = email_logs_rdd.map(lambda fields: (fields[1], 1))\n",
    "\n",
    "email_senders_rdd.collect()"
   ],
   "id": "8cac6dd9a6bd401e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sender is the second field (index 1). We create a tuple with the sender and 1 (1 email sent).\n",
    "\n",
    "Result: [('john@abc.com', 1), ('john@abc.com', 1), ('amy@abc.com', 1)]"
   ],
   "id": "4d5a81bc74e020ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Count emails sent by each user.\n",
    "\n",
    "email_counts_rdd = email_senders_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "email_counts_rdd.collect()"
   ],
   "id": "52c69b737ca3b0b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We reduce by key (sender) to count the number of emails sent by each user.\n",
    "\n",
    "Result: [('john@abc.com', 2), ('amy@abc.com', 1)]"
   ],
   "id": "26a2f3f431503dc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Identify the top 3 senders\n",
    "\n",
    "top_3_senders = email_counts_rdd.sortBy(lambda lines: lines[1], ascending=False).take(3)\n",
    "\n",
    "top_3_senders"
   ],
   "id": "f1fbbf2940367c95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We sort by the number of emails sent (index 1) from email counts rdd and take the top 3 senders.\n",
    "\n",
    "Result: [('john@abc.com', 2), ('amy@abc.com', 1)]\n",
    "\n",
    "(Result would be more relevant with a bigger dataset)"
   ],
   "id": "2022e52b478277d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "2d6ed5b1d96e8311",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 12: Online Orders Processing\n",
    "Objective: Perform filtering and grouping.\n",
    "\n",
    "Tasks:\n",
    "1. Load orders.\n",
    "2. Filter orders with status = “Shipped”.\n",
    "3. Group orders by region.\n",
    "4. Count orders per region."
   ],
   "id": "827734707ce41006"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load orders.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-OnlineOrdersProcessing\")\n",
    "orders_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/12 orders.csv\")\n",
    "\n",
    "orders_rdd.collect()"
   ],
   "id": "590c04aedb56e9bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Filter orders with status = “Shipped”.\n",
    "shipped_orders_rdd = orders_rdd.filter(lambda fields: fields[2] == \"Shipped\")\n",
    "\n",
    "shipped_orders_rdd.collect()"
   ],
   "id": "a5dd2eeef0ac392a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filter the orders with status = \"Shipped\" (index 2).",
   "id": "29149778231c09b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Group orders by region.\n",
    "grouped_orders_rdd = (\n",
    "    shipped_orders_rdd.map(lambda fields: (fields[1], fields[0]))\n",
    "    .groupByKey()\n",
    "    .map(lambda fields: (fields[0], list(fields[1])))\n",
    ")\n",
    "\n",
    "grouped_orders_rdd.collect()"
   ],
   "id": "34493e89df435800",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second field (region) and the first field (order ID) -> Group by region -> Create a tuple with the region and the list of order IDs.",
   "id": "df007b10d4b22679"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Count orders per region.\n",
    "orders_count_per_region = grouped_orders_rdd.map(\n",
    "    lambda fields: (fields[0], len(fields[1]))\n",
    ")\n",
    "\n",
    "orders_count_per_region.collect()"
   ],
   "id": "86b0d677c8054f38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the rdd from the previous step and count the number of orders per region (length of the list of order IDs).",
   "id": "e4bb6497127bce90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "c95815979b6b7ee8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 13: IoT Data Aggregation\n",
    "Objective: Work with streaming-like IoT logs.\n",
    "\n",
    "Tasks:\n",
    "1. Load device logs.\n",
    "2. Filter faulty sensor readings (<0).\n",
    "3. Count faults by sensor.\n",
    "4. Plot data (use Python later)"
   ],
   "id": "afdad35ac19662d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load device logs.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-IoTDataAggregation\")\n",
    "device_logs_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/13 iot.csv\")\n",
    "\n",
    "device_logs_rdd.collect()"
   ],
   "id": "ab29d2c23343139a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Filter faulty sensor readings (<0).\n",
    "faulty_sensor_readings_rdd = device_logs_rdd.filter(lambda fields: float(fields[2]) < 0)\n",
    "\n",
    "faulty_sensor_readings_rdd.collect()"
   ],
   "id": "8c1ae59be6d74b97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filter every faulty sensor (index 2) with a value < 0.",
   "id": "a054ac30eee97a3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Count faults by sensor.\n",
    "faults_by_sensor_rdd = faulty_sensor_readings_rdd.map(\n",
    "    lambda fields: (fields[1], 1)\n",
    ").reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "faults_by_sensor_rdd.collect()"
   ],
   "id": "63c4d7d48e374249",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take rdd from the previous step and create a tuple with the sensor ID (index 1) and 1 cpt (for 1 fault）-> Reduce by key to count the number of faults per sensor.",
   "id": "79fd846ee5a085f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Plot data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sensors = [x[0] for x in faults_by_sensor_rdd.collect()]\n",
    "fault_counts = [x[1] for x in faults_by_sensor_rdd.collect()]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(sensors, fault_counts)\n",
    "plt.xlabel(\"Sensor ID\")\n",
    "plt.ylabel(\"Number of Faults\")\n",
    "plt.title(\"Faults per Sensor\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "93541f446cf2ba46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since there is no indication on ploting, we will arbitrarily choose to plot the number of faults per sensor.\n",
    "We take the sensor ID and the number of faults -> We plot a bar chart with the sensor ID on the x-axis and the number of faults on the y-axis."
   ],
   "id": "e98ffccbffcd271b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "924a1042bb3da288",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 14: Bank Transactions Fraud Detection\n",
    "Objective: Find suspicious transactions.\n",
    "\n",
    "Tasks:\n",
    "1. Load transactions.\n",
    "2. Filter transactions over $10,000.\n",
    "3. Group by account.\n",
    "4. Identify accounts with multiple high-value transactions."
   ],
   "id": "2183b0ccac381361"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load transactions.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-BankTransactionsFraudDetection\")\n",
    "transactions_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/14 bank.csv\")\n",
    "\n",
    "transactions_rdd.collect()"
   ],
   "id": "cf986a6da85c4662",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Filter transactions over $10,000.\n",
    "high_value_transactions_rdd = transactions_rdd.filter(\n",
    "    lambda fields: float(fields[2]) > 10000\n",
    ")\n",
    "\n",
    "high_value_transactions_rdd.collect()"
   ],
   "id": "7dd6e9b2a68a77ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filter the transactions with a value > 10000 (index 2).",
   "id": "8b357610ad734a06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Group by account.\n",
    "group_byaccount_rdd = (\n",
    "    high_value_transactions_rdd.map(lambda fields: (fields[1], fields[0]))\n",
    "    .groupByKey()\n",
    "    .map(lambda fields: (fields[0], list(fields[1])))\n",
    ")\n",
    "\n",
    "group_byaccount_rdd.collect()"
   ],
   "id": "aa7affeab3221167",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second field (account) and the first field (transaction ID) -> Group by account -> Create a tuple with the account and the list of transaction IDs.",
   "id": "95bc6893022ddda6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Identify accounts with multiple high-value transactions.\n",
    "multiple_high_value_accounts_rdd = group_byaccount_rdd.filter(\n",
    "    lambda fields: len(fields[1]) > 1\n",
    ")\n",
    "\n",
    "multiple_high_value_accounts_rdd.collect()"
   ],
   "id": "ace8a41390697fc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From previous step, we filter the accounts with more than 1 transaction ID (multiple high-value transactions).",
   "id": "ee6a0d88b502e762"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "551ce1ee4ee27ce5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 15: E-commerce Product Clicks\n",
    "Objective: Perform session-based analysis.\n",
    "\n",
    "Tasks:\n",
    "1. Load click logs.\n",
    "2. Group clicks by session ID.\n",
    "3. Count number of clicks per session.\n",
    "4. Find average clicks per session"
   ],
   "id": "a1e19733a63bf045"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load click logs.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-EcommerceProductClicks\")\n",
    "click_logs_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/15 clicks.csv\")\n",
    "\n",
    "click_logs_rdd.collect()"
   ],
   "id": "d02541de754a92e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Group clicks by session ID.\n",
    "group_by_session_rdd = (\n",
    "    click_logs_rdd.map(lambda fields: (fields[0], fields[1]))\n",
    "    .groupByKey()\n",
    "    .map(lambda fields: (fields[0], list(fields[1])))\n",
    ")\n",
    "\n",
    "group_by_session_rdd.collect()"
   ],
   "id": "d226a4ae02e6c8bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the first field (session ID) and the second field (product ID) -> Group by session ID -> Create a tuple with the session ID and the list of product IDs.",
   "id": "58de55c9c1cc6da2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Count number of clicks per session.\n",
    "clicks_per_session_rdd = group_by_session_rdd.map(\n",
    "    lambda fields: (fields[0], len(fields[1]))\n",
    ")\n",
    "\n",
    "clicks_per_session_rdd.collect()"
   ],
   "id": "cf017cd77d0bc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From previous step, we take the session ID and count the number of product IDs (clicks) per session.",
   "id": "e5265dc0361f2c21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Find average clicks per session.\n",
    "average_clicks_per_session = clicks_per_session_rdd.map(lambda fields: fields[1]).mean()\n",
    "\n",
    "average_clicks_per_session"
   ],
   "id": "deeef92c19cb783",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From previous step, we take the number of clicks and calculate the average with mean().",
   "id": "c9cb0721ced760a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "3867f8bfb7884483",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 16: Exercise: Delivery Time Performance\n",
    "Objective: Measure delivery speed.\n",
    "\n",
    "Tasks:\n",
    "1. Load orders.\n",
    "2. Calculate delivery time = Delivered - Ordered.\n",
    "3. Average delivery time per region.\n",
    "4. Identify delays > average"
   ],
   "id": "55d545c31a97798d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load orders.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-DeliveryTimePerformance\")\n",
    "orders_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/16 delivery.csv\")\n",
    "\n",
    "orders_rdd.collect()"
   ],
   "id": "7c7ba9325255ba4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Calculate delivery time = Delivered - Ordered.\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def parse_dates(fields) -> (str, int):\n",
    "    region = fields[1]\n",
    "    ordered_date = datetime.strptime(fields[2], \"%Y-%m-%d\")\n",
    "    delivered_date = datetime.strptime(fields[3], \"%Y-%m-%d\")\n",
    "    delivered_time = (delivered_date - ordered_date).days\n",
    "    return region, delivered_time\n",
    "\n",
    "\n",
    "delivery_time_rdd = orders_rdd.map(parse_dates)\n",
    "\n",
    "delivery_time_rdd.collect()"
   ],
   "id": "2c412da1659839e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We create a function that will\n",
    "1. Take the region (index 1)\n",
    "2. Parse the ordered date (index 2) and delivered date (index 3) to datetime objects -> Create delivery time (in days)\n",
    "3. Return a tuple with the region and the delivery time\n",
    "\n",
    "We apply the function to the orders rdd."
   ],
   "id": "be442d48320f2fa7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Average delivery time per region.\n",
    "average_delivery_time_rdd = delivery_time_rdd.groupByKey().mapValues(\n",
    "    lambda times: sum(times) / len(times)\n",
    ")\n",
    "\n",
    "average_delivery_time_rdd.collect()"
   ],
   "id": "5ae4b044f9163fbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since we have the region and the delivery time, we group by region -> Calculate the average delivery time per region. <br>\n",
    "There is no relevant information since there's only one order by region in the dataset.\n",
    "To get a more relevant result, we could add a new line for a specific region with a higher delivery time to see if the average changes.\n",
    "\n",
    "```csv\n",
    "OrderID,Region,OrderedDate,DeliveredDate\n",
    "1,North,2025-05-01,2025-05-03\n",
    "2,South,2025-05-01,2025-05-02\n",
    "3,East,2025-05-01,2025-05-05\n",
    "4,East,2025-05-01,2025-05-30\n",
    "```\n",
    "```bash\n",
    ">>> [('North', 2.0), ('South', 1.0), ('East', 16.5)]\n",
    "```"
   ],
   "id": "c370cf7d4808418"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Identify delays > average\n",
    "average_delivery_time_dict = average_delivery_time_rdd.collectAsMap()\n",
    "delays = delivery_time_rdd.filter(\n",
    "    lambda field: field[1] > average_delivery_time_dict[field[0]]\n",
    ").map(lambda field: (field[0], field[1]))\n",
    "\n",
    "delays.collect()"
   ],
   "id": "2105fa2b868edd7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We convert the average delivery time rdd to a dictionary -> We filter the delivery time rdd to get the delays (delivery time > average delivery time) -> We create a tuple with all information about it.",
   "id": "4729cab69f0daef3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "c991b694d5b3deca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 17: Web Logs Traffic Summary\n",
    "Objective: Analyze visits per IP.\n",
    "\n",
    "Tasks:\n",
    "1. Load access logs.\n",
    "2. Map to (IP, 1).\n",
    "3. Count visits by IP.\n",
    "4. Find IPs with > 3 visits."
   ],
   "id": "f10b6de96b69bbdb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load access logs.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-WebLogsTrafficSummary\")\n",
    "access_logs_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/17 accesslog.csv\")\n",
    "\n",
    "access_logs_rdd.collect()"
   ],
   "id": "290908a8d150930c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Map to (IP, 1).\n",
    "ip_visits_rdd = access_logs_rdd.map(lambda fields: (fields[0], 1))\n",
    "\n",
    "ip_visits_rdd.collect()"
   ],
   "id": "b19bce874ca2b3e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Count visits by IP.\n",
    "ip_counts_rdd = ip_visits_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "ip_counts_rdd.collect()"
   ],
   "id": "128b09d323a70ae0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From last step, we reduce by key (IP) to count the number of visits by IP.",
   "id": "f2383b4244c099e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Find IPs with > 3 visits.\n",
    "ips_with_more_than_3_visits = ip_counts_rdd.filter(lambda fields: fields[1] > 3)\n",
    "\n",
    "ips_with_more_than_3_visits.collect()"
   ],
   "id": "c0ba96223d9a1fb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From last step, we filter the IPs with more than 3 visits. <br>\n",
    "Since there's no IP with more than 3 visits in the dataset, we get an empty list."
   ],
   "id": "9b9189a5441cf759"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "98248a76077171c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 18: Customer Lifetime Value\n",
    "Objective: Analyze total customer revenue.\n",
    "\n",
    "Tasks:\n",
    "1. Load transactions.\n",
    "2. Group by customer.\n",
    "3. Sum total spending.\n",
    "4. Rank customers by spending."
   ],
   "id": "e9deb89544050417"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load transactions.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-CustomerLifetimeValue\")\n",
    "transactions_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/18 customers.csv\")\n",
    "\n",
    "transactions_rdd.collect()"
   ],
   "id": "e7e201d2f03fbe01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Group by customer.\n",
    "group_by_customer_rdd = (\n",
    "    transactions_rdd.map(lambda fields: (fields[0], float(fields[1])))\n",
    "    .groupByKey()\n",
    "    .map(lambda fields: (fields[0], list(fields[1])))\n",
    ")\n",
    "\n",
    "group_by_customer_rdd.collect()"
   ],
   "id": "ba5b09849ffbca4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Sum total spending.\n",
    "total_spending_rdd = group_by_customer_rdd.map(\n",
    "    lambda fields: (fields[0], sum(fields[1]))\n",
    ")\n",
    "\n",
    "total_spending_rdd.collect()"
   ],
   "id": "e6ff5c737e9569a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For each customer, we take the first field (customer ID) and the second field (spending) -> We group by customer -> We create a tuple with the customer ID and the list of spending -> We sum the spending for each customer.",
   "id": "16e27f542ee2595b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Rank customers by spending.\n",
    "ranked_customers_rdd = total_spending_rdd.sortBy(\n",
    "    lambda fields: fields[1], ascending=False\n",
    ")\n",
    "\n",
    "ranked_customers_rdd.collect()"
   ],
   "id": "199976d31ea11d4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From last step, we sort by spending (index 1) to rank the customers by spending.",
   "id": "3ce9eaaedfdaa53c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "7b4aa2828713b77c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 19: Covid Case Trend Analysis\n",
    "Objective: Time-series aggregation.\n",
    "\n",
    "Tasks:\n",
    "1. Load covid daily data.\n",
    "2. Group by date.\n",
    "3. Sum total cases.\n",
    "4. Compute daily growth."
   ],
   "id": "763d6dc0045df610"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load covid daily data.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-CovidCaseTrendAnalysis\")\n",
    "covid_data_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/covid.csv\")\n",
    "\n",
    "covid_data_rdd.collect()"
   ],
   "id": "c9c6c151a38d1ef7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Group by date. Sum total cases.\n",
    "daily_cases = (\n",
    "    covid_data_rdd.map(lambda x: (x[0], int(x[2])))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .sortByKey()\n",
    ")\n",
    "\n",
    "daily_cases.collect()"
   ],
   "id": "cbe764b465a496e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Compute daily growth.\n",
    "\n",
    "daily_sorted = daily_cases.collect()\n",
    "growth = [\n",
    "    (daily_sorted[i][0], daily_sorted[i][1] - daily_sorted[i - 1][1])\n",
    "    for i in range(1, len(daily_sorted))\n",
    "]\n",
    "\n",
    "print(\"Daily Totals:\", daily_sorted)\n",
    "print(\"Daily Growth:\", growth)"
   ],
   "id": "906de0cd719e73f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We take the first field (date) and the third field (cases) -> We reduce by key (date) to sum the total cases per date -> We sort by date. <br>\n",
    "We create a list with the date and the growth (current day - previous day) -> We print the daily totals and the daily growth."
   ],
   "id": "c7bca078668265de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "676ed3f86af9e6bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 20: Real-Time Chat Logs Analysis\n",
    "Objective: Analyze chat frequency per user.\n",
    "\n",
    "Tasks:\n",
    "1. Load chat logs.\n",
    "2. Count messages per user.\n",
    "3. Identify top 3 most active users.\n",
    "4. Filter users with < 2 messages"
   ],
   "id": "5097f93e71c0bd7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load chat logs.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-RealTimeChatLogsAnalysis\")\n",
    "chat_logs_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/20 chatlog.csv\")\n",
    "\n",
    "chat_logs_rdd.collect()"
   ],
   "id": "de93f382f36ea132",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Count messages per user.\n",
    "user_messages_rdd = chat_logs_rdd.map(lambda fields: (fields[0], 1)).reduceByKey(\n",
    "    lambda a, b: a + b\n",
    ")\n",
    "\n",
    "user_messages_rdd.collect()"
   ],
   "id": "8981ca49ed9c5244",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Identify top 3 most active users.\n",
    "\n",
    "top_3_active_users = user_messages_rdd.sortBy(\n",
    "    lambda fields: fields[1], ascending=False\n",
    ").take(3)\n",
    "\n",
    "top_3_active_users"
   ],
   "id": "e732585003b04bba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Result is not relevant since there are only 3 users in the dataset.",
   "id": "f3b629a6582bbfa7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Filter users with < 2 messages\n",
    "less_than_2_messages = user_messages_rdd.filter(lambda fields: fields[1] < 2)\n",
    "\n",
    "less_than_2_messages.collect()"
   ],
   "id": "24eb9260077c7e47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "cdcc1b774acb3bc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Spark Data Frame"
   ],
   "id": "a6b6306bc98a7367"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    sum,\n",
    "    avg,\n",
    "    count,\n",
    "    max,\n",
    "    min,\n",
    "    desc,\n",
    "    lit,\n",
    "    when,\n",
    "    datediff,\n",
    ")"
   ],
   "id": "2ee4ccb69250b6c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 1: Basic Order Analysis\n",
    "Tasks\n",
    "1. Load the CSV data into a Spark DataFrame.\n",
    "2. Display the schema and first 5 rows.\n",
    "3. Calculate the total revenue (Quantity * Price) for each transaction.\n",
    "4. Group the data by Category and calculate total quantity sold per category."
   ],
   "id": "8144f87b9fb88c45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load the CSV data into a Spark DataFrame.\n",
    "spark = SparkSession.builder.appName(\"ST2CBD-Lab2-BasicOrderAnalysis\").getOrCreate()\n",
    "orders_df = spark.read.csv(\n",
    "    f\"{DIR_DATA_FRAME}/1 retail_sales.csv\", header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "orders_df.printSchema()"
   ],
   "id": "4e4c304fcb48fb3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We imported the CSV file with header and inferred schema. <br>\n",
    "We can see orders_df is a DataFrame with multiple columns (Transaction ID, Date, Store, Product, Category, Quantity, Price)."
   ],
   "id": "6bfb51e212dad525"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Display the schema and first 5 rows.\n",
    "orders_df.show(5)"
   ],
   "id": "7285cf87986704d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can select the number of rows to display by passing the number of line to the show().",
   "id": "e40b7b003bc70649"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Calculate the total revenue (Quantity * Price) for each transaction.\n",
    "orders_df = orders_df.withColumn(\"Revenue\", col(\"Quantity\") * col(\"Price\"))\n",
    "\n",
    "orders_df.select(\"TransactionID\", \"Revenue\").show(5)"
   ],
   "id": "52221fb895966638",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We created a new column (Revenue) with the total revenue for each transaction by multiplying the Quantity and Price columns.",
   "id": "f6c9ca97e9a484c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Group the data by Category and calculate total quantity sold per category.\n",
    "total_quantity_per_category = orders_df.groupBy(\"Category\").agg(\n",
    "    sum(\"Quantity\").alias(\"Total_Quantity\")\n",
    ")\n",
    "\n",
    "total_quantity_per_category.show()"
   ],
   "id": "d58e2af20b9e4fbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We grouped the data by Category and calculated the total quantity sold per category using the sum() function. <br>",
   "id": "15b1b2b11c93b664"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 2: Employee Salary Report\n",
    "\n",
    "Tasks:\n",
    "1. Load the data into a Spark DataFrame.\n",
    "2. Show all employees who joined after 2020.\n",
    "3. Find average salary per department.\n",
    "4. Display the highest-paid employee."
   ],
   "id": "7af2e546ef0c8978"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load the data into a Spark DataFrame.\n",
    "spark = SparkSession.builder.appName(\"ST2CBD-Lab2-EmployeeSalaryReport\").getOrCreate()\n",
    "employees_df = spark.read.csv(\n",
    "    f\"{DIR_DATA_FRAME}/2 employees.csv\", header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# employees_df.printSchema()\n",
    "employees_df.show()"
   ],
   "id": "daf81e39e5f4992e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Show all employees who joined after 2020.\n",
    "employees_after_2020 = employees_df.filter(col(\"JoinDate\") > \"2020-01-01\")\n",
    "\n",
    "employees_after_2020.show()"
   ],
   "id": "52edb9c49ce2b675",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Find average salary per department.\n",
    "average_salary_per_department = employees_df.groupBy(\"Department\").agg(\n",
    "    avg(\"Salary\").alias(\"Average_Salary\")\n",
    ")\n",
    "\n",
    "average_salary_per_department.show()"
   ],
   "id": "9d87017bbcb820dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We grouped by Department and calculated the average salary per department using the avg() function.",
   "id": "eb337714a5f1253e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Display the highest-paid employee.\n",
    "highest_paid_employee = employees_df.orderBy(desc(\"Salary\")).first()\n",
    "\n",
    "highest_paid_employee"
   ],
   "id": "3a98994de498e5ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the highest salary by ordering the DataFrame by Salary in descending order (using orderBy and desc) and taking the first row. <br>",
   "id": "ec307daa6e798c0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 3: Bank Transactions Overview\n",
    "Tasks:\n",
    "\n",
    "1. Load the data and infer schema.\n",
    "2. Calculate total deposits and withdrawals.\n",
    "3. Show total transaction amount per customer.\n",
    "4. Find the branch with the highest transaction value"
   ],
   "id": "d90cf8cad5a8d022"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load the data and infer schema.\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"ST2CBD-Lab2-BankTransactionsOverview\"\n",
    ").getOrCreate()\n",
    "bank_transactions_df = spark.read.csv(\n",
    "    f\"{DIR_DATA_FRAME}/3 transactions.csv\", header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# bank_transactions_df.printSchema()\n",
    "bank_transactions_df.show()"
   ],
   "id": "4f9c568960c14cd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Calculate total deposits and withdrawals.\n",
    "\n",
    "total_deposits = bank_transactions_df.filter(col(\"Type\") == \"Deposit\").agg(\n",
    "    sum(\"Amount\").alias(\"Total Deposits\")\n",
    ")\n",
    "\n",
    "total_withdrawals = bank_transactions_df.filter(col(\"Type\") == \"Withdrawal\").agg(\n",
    "    sum(\"Amount\").alias(\"Total Withdrawals\")\n",
    ")\n",
    "\n",
    "total_deposits.show()\n",
    "total_withdrawals.show()"
   ],
   "id": "dc22195046781237",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filtered the DataFrame by Type (Deposit or Withdrawal) and calculated the total amount using the sum() function.",
   "id": "8724d83738472b88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Show total transaction amount per customer.\n",
    "# If Deposit, +Amount\n",
    "# If Withdrawal, -Amount\n",
    "\n",
    "bank_transactions_df = bank_transactions_df.withColumn(\n",
    "    \"Total Transaction Ammount\",\n",
    "    when(col(\"Type\") == \"Deposit\", col(\"Amount\"))\n",
    "    .when(col(\"Type\") == \"Withdrawal\", -col(\"Amount\"))\n",
    "    .otherwise(0),\n",
    ")\n",
    "\n",
    "total_per_customer = (\n",
    "    bank_transactions_df.groupBy(\"CustomerID\")\n",
    "    .agg(sum(\"Total Transaction Ammount\").alias(\"Current Balance\"))\n",
    "    .orderBy(desc(\"Current Balance\"))\n",
    ")\n",
    "\n",
    "total_per_customer.show()"
   ],
   "id": "73915550c984ee72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the first time, we created a new column (Total Transaction Amount) with the total transaction amount for each customer by using when() to check if the transaction is a Deposit or Withdrawal. <br>\n",
    "Then, we grouped by CustomerID and calculated the total transaction amount per customer using the sum() function. <br>\n",
    "We ordered the DataFrame by Current Balance in descending order to show the customers with the highest balance first."
   ],
   "id": "45af0bd7e9b385dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Find the branch with the highest transaction value\n",
    "\n",
    "highest_transaction_branch = (\n",
    "    bank_transactions_df.groupBy(\"Branch\")\n",
    "    .agg(sum(\"Amount\").alias(\"Total Transaction Amount\"))\n",
    "    .orderBy(desc(\"Total Transaction Amount\"))\n",
    "    .first()\n",
    ")\n",
    "\n",
    "highest_transaction_branch"
   ],
   "id": "e3503097ef4f7fa3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We groupBy Branch column and calculate the total transaction amount using the sum() function. <br>\n",
    "We order the DataFrame by Total Transaction Amount in descending order and take the first row to get the branch with the highest transaction value."
   ],
   "id": "d187157ef73502ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 4: Hospital Patient Records\n",
    "\n",
    "Tasks:\n",
    "1. Load and display data.\n",
    "2. Calculate the length of stay for each patient.\n",
    "3. Find average stay duration by department.\n",
    "4. Count male and female patients per department."
   ],
   "id": "d94c8b2e4b203aa4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load and display data.\n",
    "spark = SparkSession.builder.appName(\"ST2CBD-Lab2-HospitalPatientRecords\").getOrCreate()\n",
    "hospital_records_df = spark.read.csv(\n",
    "    f\"{DIR_DATA_FRAME}/4 patients.csv\", header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# hospital_records_df.printSchema()\n",
    "hospital_records_df.show()"
   ],
   "id": "8454097936c085b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Calculate the length of stay for each patient.\n",
    "hospital_records_df = hospital_records_df.withColumn(\n",
    "    \"Length of Stay\", datediff(col(\"DischargeDate\"), col(\"AdmissionDate\"))\n",
    ")\n",
    "\n",
    "hospital_records_df.show()"
   ],
   "id": "29d1d0fd049597cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Find average stay duration by department.\n",
    "average_stay_duration = hospital_records_df.groupBy(\"Department\").agg(\n",
    "    avg(\"Length of Stay\").alias(\"Average Stay Duration\")\n",
    ")\n",
    "\n",
    "average_stay_duration.show()"
   ],
   "id": "da8e6b0044da2aa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Count male and female patients per department.\n",
    "gender_count_per_department_df = hospital_records_df.groupBy(\n",
    "    \"Department\", \"Gender\"\n",
    ").count()\n",
    "\n",
    "gender_count_per_department_df.show()"
   ],
   "id": "267ddca69d97942b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 5: Supermarket Inventory Management\n",
    "\n",
    "Tasks:\n",
    "1. Identify products below their reorder level.\n",
    "2. Calculate total stock value per category.\n",
    "3. Add a column for total value (Stock * Price).\n",
    "4. Show top 3 products by stock."
   ],
   "id": "9c828583e3d50fdf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load CSV\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"ST2CBD-Lab2-SupermarketInventoryManagement\"\n",
    ").getOrCreate()\n",
    "inventory_df = spark.read.csv(\n",
    "    f\"{DIR_DATA_FRAME}/5 inventory.csv\", header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# inventory_df.printSchema()\n",
    "inventory_df.show()"
   ],
   "id": "1fab825fbde5c60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Identify products below their reorder level.\n",
    "low_stock_products = inventory_df.filter(col(\"Stock\") < col(\"ReorderLevel\"))\n",
    "\n",
    "low_stock_products.show()"
   ],
   "id": "f2dcebd8c7dde0cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We compare the **Stock** and **ReorderLevel** columns to filter the products below their reorder level.",
   "id": "6343aa8c10e0bdd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Calculate total stock value per category.\n",
    "\n",
    "total_stock_value_per_category = inventory_df.groupBy(\"Category\").agg(\n",
    "    sum(col(\"Stock\") * col(\"Price\")).alias(\"Total Stock Value\")\n",
    ")\n",
    "\n",
    "total_stock_value_per_category.show()"
   ],
   "id": "10c74f2c7547619",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We grouped the df by **Category** and calculated the total stock value per category using the sum() function from **Stock** and **Price** column.",
   "id": "fef60f76ff61b2b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Add a column for total value (Stock * Price).\n",
    "\n",
    "inventory_df = inventory_df.withColumn(\"Total Value\", col(\"Stock\") * col(\"Price\"))\n",
    "\n",
    "inventory_df.show()"
   ],
   "id": "f8741d1e14f0107b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We simply created a new column (Total Value) with the total value depending on the **Stock** and **Price** columns.",
   "id": "27a9809d96acad20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Show top 3 products by stock.\n",
    "top_3_products_by_stock = inventory_df.orderBy(desc(\"Stock\")).limit(3)\n",
    "\n",
    "top_3_products_by_stock.show()"
   ],
   "id": "ba994a0fac847371",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We ordered the DataFrame by **Stock** in descending order and limited the result to 3 rows to show the top 3 products by stock.",
   "id": "f8ce1926b2d68af9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Spark Pandas"
   ],
   "id": "14e376c6a844e48f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "b12f285148c85aaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 1: Analyze Student Exam Scores\n",
    "Objective: Understand basic Spark with Pandas operations – reading, filtering, and computing statistics.\n",
    "\n",
    "Tasks:\n",
    "1. Read the data into a Spark DataFrame and convert to Pandas.\n",
    "2. Calculate the average score for each subject.\n",
    "3. Filter students with scores above 80.\n",
    "4. Visualize scores using a bar plot."
   ],
   "id": "1f4bee3eb5bd0de4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Read the data into a Spark DataFrame and convert to Pandas.\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"ST2CBD-Lab2-AnalyzeStudentExamScores\"\n",
    ").getOrCreate()\n",
    "students_df = spark.read.csv(\n",
    "    f\"{DIR_SPARK_PANDAS}/1 scores.csv\", header=True, inferSchema=True\n",
    ")\n",
    "students_df = students_df.toPandas()\n",
    "\n",
    "students_df"
   ],
   "id": "d68e0286696ce932",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Calculate the average score for each subject.\n",
    "average_scores = students_df.groupby(\"subject\")[\"score\"].mean().reset_index()\n",
    "\n",
    "average_scores"
   ],
   "id": "43e51b8a67ad5bff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We grouped the df by **subject** and calculated the average **score** using the mean() function.",
   "id": "d7c9378ca4b22bd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Filter students with scores above 80.\n",
    "students_above_80 = students_df[students_df[\"score\"] > 80][\n",
    "    [\"name\", \"subject\", \"score\"]\n",
    "].reset_index(drop=True)\n",
    "\n",
    "students_above_80"
   ],
   "id": "1cf1a6a7553a7a9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Visualize all scores using a bar plot.\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(students_df[\"name\"], students_df[\"score\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Student Name\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Student Exam Scores\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "1b597e8478f60725",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 2: Employee Working Hours Log\n",
    "Objective: Clean and aggregate data using Spark with Pandas.\n",
    "\n",
    "Tasks:\n",
    "1. Compute daily hours worked.\n",
    "2. Normalize to 8-hour shifts.\n",
    "3. Highlight overtime workers.\n",
    "4. Export final table to CSV using Pandas API."
   ],
   "id": "ad40be2cf8a0e2d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load CSV\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"ST2CBD-Lab2-EmployeeWorkingHoursLog\"\n",
    ").getOrCreate()\n",
    "employees_df = spark.read.csv(\n",
    "    f\"{DIR_SPARK_PANDAS}/2 working_hours.csv\", header=True, inferSchema=True\n",
    ")\n",
    "employees_df = employees_df.toPandas()\n",
    "employees_df"
   ],
   "id": "e1adea87a0773dce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Compute daily hours worked.\n",
    "employees_df[\"daily_hours\"] = (\n",
    "    pd.to_datetime(employees_df[\"checkout\"]) - pd.to_datetime(employees_df[\"checkin\"])\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "employees_df"
   ],
   "id": "559c2bce497b14ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Normalize to 8-hour shifts.\n",
    "employees_df[\"normalized_hours\"] = employees_df[\"daily_hours\"] / 8\n",
    "\n",
    "employees_df"
   ],
   "id": "2e78865c4204c8e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Highlight overtime workers.\n",
    "employees_df[\"overtime\"] = employees_df[\"normalized_hours\"].apply(\n",
    "    lambda x: \"Yes\" if x > 1 else \"No\"\n",
    ")\n",
    "employees_df"
   ],
   "id": "3c273678027d40de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Export final table to CSV using Pandas API.\n",
    "employees_df.to_csv(f\"{DIR_SPARK_PANDAS}/2 working_hours_final.csv\", index=False)"
   ],
   "id": "57d9d0ecf1342064",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Final CSV:\n",
    "\n",
    "```csv\n",
    "emp_id,name,checkin,checkout,daily_hours,normalized_hours,overtime\n",
    "301,Sam,2025-05-14 09:00:00,2025-05-14 17:00:00,8.0,1.0,No\n",
    "302,Rita,2025-05-14 09:30:00,2025-05-14 18:15:00,8.75,1.09375,Yes\n",
    "303,Mike,2025-05-14 08:45:00,2025-05-14 16:50:00,8.083333333333334,1.0104166666666667,Yes\n",
    "```"
   ],
   "id": "da8e5e54f97bb7a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 3: E-commerce Cart Abandonment Analysis\n",
    "Objective: Identify patterns and segment users who abandon their shopping carts.\n",
    "\n",
    "Tasks:\n",
    "1. Identify sessions where a product was added to cart but not purchased.\n",
    "2. Count total abandoned cart sessions per user.\n",
    "3. Visualize abandonment frequency by hour.\n",
    "4. Recommend targeted re-engagement strategies for top 2 users"
   ],
   "id": "18a089a3dfa07982"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load CSV\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"ST2CBD-Lab2-EcommerceCartAbandonmentAnalysis\"\n",
    ").getOrCreate()\n",
    "\n",
    "cart_data_df = spark.read.csv(\n",
    "    f\"{DIR_SPARK_PANDAS}/3 cart.csv\", header=True, inferSchema=True\n",
    ")\n",
    "cart_data_df = cart_data_df.toPandas()\n",
    "\n",
    "cart_data_df"
   ],
   "id": "76717304ddeb85bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Identify sessions where a product was added to cart but not purchased.\n",
    "sessions_with_purchase = set(\n",
    "    cart_data_df[cart_data_df[\"event_type\"] == \"purchase\"][\"session_id\"]\n",
    ")\n",
    "\n",
    "abandoned_sessions = cart_data_df[\n",
    "    (cart_data_df[\"event_type\"] == \"add_to_cart\")\n",
    "    & (~cart_data_df[\"session_id\"].isin(sessions_with_purchase))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "abandoned_sessions"
   ],
   "id": "7330775e02485f7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Count total abandoned cart sessions per user.\n",
    "abandoned_count_per_user = (\n",
    "    abandoned_sessions.groupby(\"user_id\")[\"session_id\"].nunique().reset_index()\n",
    ")\n",
    "abandoned_count_per_user.columns = [\"user_id\", \"abandoned_cart_count\"]\n",
    "abandoned_count_per_user"
   ],
   "id": "ab8b384aaf1fa96a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We groupBy **user_id** and count the number of unique **session_id** for each user.",
   "id": "a4bb02c4e5c2c6c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Visualize abandonment frequency by hour.\n",
    "abandoned_sessions[\"hour\"] = pd.to_datetime(abandoned_sessions[\"timestamp\"]).dt.hour\n",
    "abandonment_by_hour = (\n",
    "    abandoned_sessions.groupby(\"hour\")[\"session_id\"].count().reset_index()\n",
    ")\n",
    "abandonment_by_hour.columns = [\"hour\", \"abandoned_cart_count\"]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(\n",
    "    abandonment_by_hour[\"hour\"],\n",
    "    abandonment_by_hour[\"abandoned_cart_count\"],\n",
    "    color=\"orange\",\n",
    ")\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Abandoned Cart Count\")\n",
    "plt.title(\"Abandoned Cart Frequency by Hour\")\n",
    "plt.xticks(range(24))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a739f5003b5fe42d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We fetched the hour from the timestamp and grouped by hour to count the number of abandoned cart sessions.\n",
    "We plotted a bar chart with the hour on the x-axis and the number of abandoned cart sessions on the y-axis."
   ],
   "id": "c633f055ae1abb06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Recommend targeted re-engagement strategies for top 2 users\n",
    "top_2_users = abandoned_count_per_user.nlargest(2, \"abandoned_cart_count\")\n",
    "for _, row in top_2_users.iterrows():\n",
    "    user_id = row[\"user_id\"]\n",
    "    abandoned_count = row[\"abandoned_cart_count\"]\n",
    "    print(\n",
    "        f\"User {user_id} abandoned {abandoned_count} carts. Recommend sending personalized emails with discounts.\"\n",
    "    )"
   ],
   "id": "3c7faa37ee522ad1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 4: IoT Sensor Anomaly Detection\n",
    "Objective: Detect temperature anomalies using rolling mean and standard deviation.\n",
    "\n",
    "Tasks:\n",
    "1. Convert timestamp to datetime and sort data.\n",
    "2. Apply rolling window mean and std dev over last 3 readings.\n",
    "3. Flag readings outside 2 standard deviations as anomalies.\n",
    "4. Plot anomalies over time."
   ],
   "id": "e708221dbb719e05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load CSV\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"ST2CBD-Lab2-IoTSensorAnomalyDetection\"\n",
    ").getOrCreate()\n",
    "iot_data_df = spark.read.csv(\n",
    "    f\"{DIR_SPARK_PANDAS}/4 temp.csv\", header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "iot_data_df = iot_data_df.toPandas()\n",
    "\n",
    "iot_data_df"
   ],
   "id": "d2fddbb1c3f0a56f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Convert timestamp to datetime and sort data.\n",
    "iot_data_df[\"timestamp\"] = pd.to_datetime(iot_data_df[\"timestamp\"])\n",
    "iot_data_df = iot_data_df.sort_values(by=\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "iot_data_df"
   ],
   "id": "5404121bebc5b832",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We converted the **timestamp** column to datetime format and sorted the df by timestamp. <br>\n",
    "Since the timestamp column is already ordered, no difference is shown in the output."
   ],
   "id": "e60a5a39749db607"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Apply rolling window mean and std dev over last 3 readings.\n",
    "iot_data_df[\"rolling_mean\"] = iot_data_df[\"temperature\"].rolling(window=3).mean()\n",
    "iot_data_df[\"rolling_std\"] = iot_data_df[\"temperature\"].rolling(window=3).std()\n",
    "\n",
    "iot_data_df"
   ],
   "id": "51aa51f1b66260ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To make the rolling mean and std dev, we used the rolling() function with a window of 3 readings. <br>\n",
    "Because of need of 3 readings to calculate the mean and std dev, the first 2 rows will have NaN values."
   ],
   "id": "dadb05059f7ed3db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Flag readings outside 2 standard deviations as anomalies.\n",
    "\n",
    "iot_data_df[\"lower_bound\"] = (\n",
    "    iot_data_df[\"rolling_mean\"] - 2 * iot_data_df[\"rolling_std\"]\n",
    ")\n",
    "iot_data_df[\"upper_bound\"] = (\n",
    "    iot_data_df[\"rolling_mean\"] + 2 * iot_data_df[\"rolling_std\"]\n",
    ")\n",
    "\n",
    "iot_data_df[\"anomaly\"] = iot_data_df.apply(\n",
    "    lambda row: (\n",
    "        \"Yes\"\n",
    "        if (\n",
    "            row[\"temperature\"] < row[\"lower_bound\"]\n",
    "            or row[\"temperature\"] > row[\"upper_bound\"]\n",
    "        )\n",
    "        else \"No\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "iot_data_df"
   ],
   "id": "3750edd11b366648",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "No anomalies are detected in the dataset. <br>",
   "id": "6600ce16440c2229"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Plot anomalies over time.\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(iot_data_df[\"timestamp\"], iot_data_df[\"temperature\"], label=\"Temperature\")\n",
    "\n",
    "anomalies = iot_data_df[iot_data_df[\"anomaly\"] == \"Yes\"]\n",
    "plt.scatter(\n",
    "    anomalies[\"timestamp\"],\n",
    "    anomalies[\"temperature\"],\n",
    "    color=\"red\",\n",
    "    label=\"Anomalies\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Temperature\")\n",
    "plt.title(\"IoT Sensor Temperature with Anomalies Highlighted\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "id": "4ebfa4a3dc6af7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 5: Social Media Sentiment Tracker\n",
    "Objective: Track and visualize daily sentiment polarity from user posts.\n",
    "\n",
    "Tasks:\n",
    "1. Compute sentiment polarity using TextBlob on posts.\n",
    "2. Aggregate average sentiment by date.\n",
    "3. Classify posts as Positive/Negative/Neutral.\n",
    "4. Visualize sentiment over time."
   ],
   "id": "54b75218a91a6a50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load CSV\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"ST2CBD-Lab2-SocialMediaSentimentTracker\"\n",
    ").getOrCreate()\n",
    "social_media_df = spark.read.csv(\n",
    "    f\"{DIR_SPARK_PANDAS}/5 tracker.csv\", header=True, inferSchema=True\n",
    ")\n",
    "social_media_df = social_media_df.toPandas()\n",
    "\n",
    "social_media_df"
   ],
   "id": "48ba1c9d14b34a21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Compute sentiment polarity using TextBlob on posts.\n",
    "from textblob import TextBlob\n",
    "\n",
    "social_media_df[\"polarity\"] = social_media_df[\"post\"].apply(\n",
    "    lambda x: TextBlob(x).sentiment.polarity\n",
    ")\n",
    "\n",
    "social_media_df"
   ],
   "id": "b21ff1922aac3102",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We used the TextBlob library, from column **post**, we compute the sentiment polarity and created a new column (polarity) with the result. <br>",
   "id": "3d03542d17ba1e53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Aggregate average sentiment by date.\n",
    "social_media_df[\"date\"] = pd.to_datetime(social_media_df[\"date\"])\n",
    "average_sentiment_by_date = (\n",
    "    social_media_df.groupby(\"date\")[\"polarity\"].mean().reset_index()\n",
    ")\n",
    "\n",
    "average_sentiment_by_date"
   ],
   "id": "d88a73caca92227b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the first part, we converted the **date** column to datetime format. <br>\n",
    "Then, we group by **date** and calculated the average sentiment polarity using the mean() function."
   ],
   "id": "cd2b90e34d3fa393"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Classify posts as Positive/Negative/Neutral.\n",
    "def classify_sentiment(polarity):\n",
    "    if polarity > 0:\n",
    "        return \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "\n",
    "social_media_df[\"sentiment\"] = social_media_df[\"polarity\"].apply(classify_sentiment)\n",
    "\n",
    "social_media_df"
   ],
   "id": "c36d114a7eb344ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We created a function to classify the sentiment based on the polarity value. <br>\n",
    "Positive if > 0, Negative if < 0, Neutral if = 0. <br>\n",
    "Then, we applied the function to the **polarity** column and created a new column (sentiment) with the result."
   ],
   "id": "b4e787d5ddbd7db1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Visualize sentiment over time.\n",
    "average_sentiment_by_date[\"sentiment\"] = average_sentiment_by_date[\"polarity\"].apply(\n",
    "    classify_sentiment\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    average_sentiment_by_date[\"date\"],\n",
    "    average_sentiment_by_date[\"polarity\"],\n",
    "    marker=\"o\",\n",
    "    label=\"Average Polarity\",\n",
    ")\n",
    "plt.title(\"Average Sentiment Polarity Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Average Polarity\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\", label=\"Neutral\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "805d62f5af21ddff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We plotted the average sentiment polarity over time with the date on the x-axis and the average polarity on the y-axis. <br>",
   "id": "d0788e12d525074"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
