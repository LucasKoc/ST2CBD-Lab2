{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:50.915280Z",
     "start_time": "2025-05-14T18:09:50.841561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pip install pyspark\n",
    "from pyspark import SparkContext, RDD\n",
    "\n",
    "DIR_PATH = \"Sparks_dataset/Sparks_dataset\"\n",
    "DIR_CONTEXT = DIR_PATH + \"/Context\"\n",
    "DIR_DATA_FRAME = DIR_PATH + \"/Data Frame\"\n",
    "DIR_SPARK_PANDAS = DIR_PATH + \"/Spark Pandas\""
   ],
   "id": "95868fa35c1cb0a2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 1 - Basic Order Analysis\n",
    "Tasks:\n",
    "\n",
    "1. Load the dataset into Spark using SparkContext and textFile().\n",
    "2. Count the total number of orders.\n",
    "3. Calculate the total revenue from all orders.\n",
    "4. Find the number of unique customers."
   ],
   "id": "e67fddf6f9076e31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:52.809646Z",
     "start_time": "2025-05-14T18:09:50.991699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load the dataset into Spark using SparkContext and textFile().\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-OrderAnalysis\")\n",
    "\n",
    "# Load the dataset\n",
    "salesdata_rdd = sc.textFile(f\"{DIR_CONTEXT}/salesdata.csv.txt\")"
   ],
   "id": "c7cc47fcb82bc656",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/14 20:09:51 WARN Utils: Your hostname, MacBook-Pro-de-Lucas.local resolves to a loopback address: 127.0.0.1; using 10.0.0.18 instead (on interface en0)\n",
      "25/05/14 20:09:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/14 20:09:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:53.582870Z",
     "start_time": "2025-05-14T18:09:52.816739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Count the total number of orders.\n",
    "# Because the file has 2 header lines, we need to remove them before counting\n",
    "header = salesdata_rdd.take(2)\n",
    "\n",
    "# We remove the header lines from the RDD\n",
    "salesdata_rdd = salesdata_rdd.filter(\n",
    "    lambda line: line != header[0] and line != header[1]\n",
    ")\n",
    "\n",
    "# Count the total number of orders\n",
    "total_orders = salesdata_rdd.count()\n",
    "\n",
    "total_orders"
   ],
   "id": "86c575dae4b985c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the CSV file, we have 2 header lines. In order to count the total number of orders, we need to remove the header lines from the RDD.\n",
    "\n",
    "Total number of orders: 10"
   ],
   "id": "1a508f782e74823"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:53.632800Z",
     "start_time": "2025-05-14T18:09:53.592836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Calculate the total revenue from all orders.\n",
    "# Index of OrderAmount is 2\n",
    "\n",
    "sales_revenues = salesdata_rdd.map(lambda line: float(line.split(\",\")[2]))\n",
    "\n",
    "total_revenue = sales_revenues.sum()\n",
    "\n",
    "total_orders"
   ],
   "id": "b34c4792434cd3ec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Total orders = 10",
   "id": "6d246ad2ccd2ef50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:53.849077Z",
     "start_time": "2025-05-14T18:09:53.636834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Find the number of unique customers.\n",
    "# Index of CustomerID is 1\n",
    "\n",
    "unique_customers = salesdata_rdd.map(lambda line: line.split(\",\")[1]).distinct()\n",
    "\n",
    "total_unique_customers = unique_customers.count()\n",
    "\n",
    "total_unique_customers"
   ],
   "id": "8fe76ddec7eeefd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Total unique customers = 6",
   "id": "74470cb99553048e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:54.849147Z",
     "start_time": "2025-05-14T18:09:53.865830Z"
    }
   },
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "99fa81529213e507",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 2 - Department Salary Stats\n",
    "Tasks:\n",
    "\n",
    "1. Load employee data and skip header.\n",
    "2. Find total number of employees.\n",
    "3. Calculate average salary per department.\n",
    "4. Find department with highest total salary."
   ],
   "id": "f01eebc57846386b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:55.534784Z",
     "start_time": "2025-05-14T18:09:54.857515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load employee data and skip header.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-DepartmentSalaryStats\")\n",
    "\n",
    "# Load the dataset\n",
    "salesdata_rdd = sc.textFile(f\"{DIR_CONTEXT}/2 salary.csv\")\n",
    "\n",
    "# Skip header\n",
    "header = salesdata_rdd.take(1)\n",
    "\n",
    "salesdata_rdd = salesdata_rdd.filter(lambda line: line != header[0])\n",
    "\n",
    "salesdata_rdd.take(1)"
   ],
   "id": "f74f352ed5802b88",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E101,Alice,Sales,50000']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Result should not be the header and be the first line: ['E101,Alice,Sales,50000']",
   "id": "a550e53efe49c99b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:55.614716Z",
     "start_time": "2025-05-14T18:09:55.558766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Find total number of employees.\n",
    "\n",
    "total_employees = salesdata_rdd.count()\n",
    "\n",
    "total_employees"
   ],
   "id": "227aa7a1024b4fd5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Total employees = 8",
   "id": "4edc37aafcd4a52c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:55.701071Z",
     "start_time": "2025-05-14T18:09:55.636447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Calculate average salary per department.\n",
    "\n",
    "average_salary_per_department = (\n",
    "    salesdata_rdd.map(lambda line: (line.split(\",\")[2], float(line.split(\",\")[3])))\n",
    "    .groupByKey()\n",
    "    .mapValues(lambda salaries: sum(salaries) / len(salaries))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "average_salary_per_department"
   ],
   "id": "cc436f0fea17412f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sales', 54000.0), ('Marketing', 56500.0), ('IT', 69000.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Departements average salaries:\n",
    "- Sales: 54000\n",
    "- Marketing: 56500\n",
    "- IT: 69000"
   ],
   "id": "9725d09411ebae4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:55.769773Z",
     "start_time": "2025-05-14T18:09:55.714315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Find department with the highest total salary.\n",
    "\n",
    "highest_salary_department = (\n",
    "    salesdata_rdd.map(lambda line: (line.split(\",\")[2], float(line.split(\",\")[3])))\n",
    "    .groupByKey()\n",
    "    .mapValues(lambda salaries: sum(salaries))\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "    .first()\n",
    ")\n",
    "\n",
    "highest_salary_department"
   ],
   "id": "44615ce9aac8dfe1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('IT', 207000.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:56.770124Z",
     "start_time": "2025-05-14T18:09:55.782922Z"
    }
   },
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "55d1a074e45e93f3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 3: Web Traffic Summary\n",
    "Dataset: web_logs.txt\n",
    "\n",
    "Tasks:\n",
    "1. Load the data and parse it into fields.\n",
    "2. Count total number of visits per URL.\n",
    "3. Find unique users per URL.\n",
    "4. Determine peak hour for traffic."
   ],
   "id": "6edab637bed4684e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:57.362577Z",
     "start_time": "2025-05-14T18:09:56.777719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load the data and parse it into fields.\n",
    "\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-WebTrafficSummary\")\n",
    "web_logs_rdd = sc.textFile(f\"{DIR_CONTEXT}/3 web_logs.txt\")\n",
    "\n",
    "# Skip header\n",
    "header = web_logs_rdd.take(1)\n",
    "web_logs_rdd = web_logs_rdd.filter(lambda line: line != header[0])\n",
    "\n",
    "# Parse the data into fields\n",
    "web_logs_rdd = web_logs_rdd.map(lambda line: line.split(\",\"))"
   ],
   "id": "785b94fe85957533",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:57.443124Z",
     "start_time": "2025-05-14T18:09:57.371629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Count total number of visits per URL.\n",
    "\n",
    "url_visits = (\n",
    "    web_logs_rdd.map(lambda fields: (fields[1], 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "url_visits"
   ],
   "id": "c4677f721b4c86f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/home', 6), ('/product', 2), ('/contact', 1), ('/about', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We take the URL and the number of visits -> Reduce by key to count visits (line = 1 visit).\n",
    "\n",
    "Result: [('/home', 6), ('/product', 2), ('/contact', 1), ('/about', 1)]"
   ],
   "id": "63f7a4dff22daf7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:57.518981Z",
     "start_time": "2025-05-14T18:09:57.463895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Find unique users per URL.\n",
    "\n",
    "unique_users_per_url = (\n",
    "    web_logs_rdd.map(lambda fields: (fields[1], fields[0]))\n",
    "    .distinct()\n",
    "    .map(lambda x: (x[0], 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "unique_users_per_url"
   ],
   "id": "3632b9b2735e74d2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/home', 4), ('/product', 2), ('/contact', 1), ('/about', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First we take the URL and the user ID -> Remove duplicates -> Reduce by key to count unique users.\n",
    "\n",
    "Result: [('/home', 4), ('/product', 2), ('/contact', 1), ('/about', 1)]"
   ],
   "id": "a64a55cd22098268"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:57.608476Z",
     "start_time": "2025-05-14T18:09:57.543141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Determine peak hour for traffic.\n",
    "from datetime import datetime\n",
    "\n",
    "hour_counts = web_logs_rdd.map(\n",
    "    lambda fields: (datetime.strptime(fields[2], \"%Y-%m-%d %H:%M:%S\").hour, 1)\n",
    ").reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "peak_hour = hour_counts.sortBy(lambda x: x[1], ascending=False).first()\n",
    "\n",
    "peak_hour"
   ],
   "id": "45aa309e2f180eec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We take the field index 2 (Timestamp) and convert it to a datetime object -> Get the hour -> Reduce by key to count visits per hour.\n",
    "\n",
    "After that we sort by the number of visits and take the first one.\n",
    "\n",
    "Result: (13, 2) -> 13:00 w/ 2 visits"
   ],
   "id": "583ec38036cfe7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:58.609155Z",
     "start_time": "2025-05-14T18:09:57.634084Z"
    }
   },
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "329c7e87d5d781df",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 4: Product Rating Analysis\n",
    "Tasks: Dataset: product_reviews.csv\n",
    "1. Load and parse the review data.\n",
    "2. Calculate the average rating per product.\n",
    "3. Count number of reviews per product.\n",
    "4. Find products with all 5-star reviews."
   ],
   "id": "d4e368e4eb5e0ebf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:58.618682Z",
     "start_time": "2025-05-14T18:09:58.616815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_rdd_csv(file_path, header_lines=1) -> RDD[str]:\n",
    "    rdd = sc.textFile(file_path)\n",
    "    header = rdd.take(header_lines)\n",
    "    rdd = rdd.filter(lambda line: line not in header)\n",
    "\n",
    "    rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "    return rdd"
   ],
   "id": "a1da1dc25c43d723",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Created a function for parsing input files. It takes the file path and the number of header lines to skip (header). It returns an RDD with the parsed data.",
   "id": "73ed62f052e0348e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:59.245127Z",
     "start_time": "2025-05-14T18:09:58.658272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load and parse the review data.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-ProductRatingAnalysis\")\n",
    "production_reviews_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/4 product_reviews.csv\")"
   ],
   "id": "7306bfedeec312bf",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:59.326268Z",
     "start_time": "2025-05-14T18:09:59.254282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Calculate the average rating per product\n",
    "average_rating_per_product = (\n",
    "    production_reviews_rdd.map(lambda fields: (fields[1], float(fields[2])))\n",
    "    .groupByKey()\n",
    "    .mapValues(lambda ratings: sum(ratings) / len(ratings))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "average_rating_per_product"
   ],
   "id": "e7ca0319ccb11219",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('P001', 5.0), ('P002', 4.0), ('P003', 4.0), ('P004', 5.0), ('P005', 4.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second (product) and third field (score) -> Group by product -> Calculate the average score.",
   "id": "fae3f68d8b027e7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:59.375966Z",
     "start_time": "2025-05-14T18:09:59.337827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Count number of reviews per product\n",
    "\n",
    "reviews_per_product = (\n",
    "    production_reviews_rdd.map(lambda fields: (fields[1], 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "reviews_per_product"
   ],
   "id": "f97835dfd8a0e184",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('P001', 3), ('P002', 2), ('P003', 2), ('P004', 2), ('P005', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second field (product) and count the number of reviews (each line correspond to one product review).",
   "id": "df558353ba2c972d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:59.437273Z",
     "start_time": "2025-05-14T18:09:59.400636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Find products with all 5-star reviews.\n",
    "\n",
    "five_star_products = (\n",
    "    production_reviews_rdd.map(lambda fields: (fields[1], float(fields[2])))\n",
    "    .groupByKey()\n",
    "    .filter(lambda x: all(rating == 5.0 for rating in x[1]))\n",
    "    .map(lambda x: x[0])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "five_star_products"
   ],
   "id": "8117518fa47e9efd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P001', 'P004']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second field (product) and the third field (score) -> Group by product -> Filter products with all 5-star reviews",
   "id": "ed1767d18fb97e8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:10:00.434856Z",
     "start_time": "2025-05-14T18:09:59.460658Z"
    }
   },
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "4a13d5a10703d42",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 5: Movie Ratings Analytics\n",
    "Objective: Learn to read a CSV file, initialize SparkContext, and perform basic operations.\n",
    "\n",
    "Tasks:\n",
    "1. Load the movies.csv using SparkContext.textFile.\n",
    "2. Count the total number of movies.\n",
    "3. Filter movies with rating >= 4.\n",
    "4. Display the top 5 movies by rating."
   ],
   "id": "347df52a5618099e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:10:01.012257Z",
     "start_time": "2025-05-14T18:10:00.442696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load the movies.csv using SparkContext.textFile.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-MovieRatingsAnalytics\")\n",
    "movies_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/5 movies.csv\")"
   ],
   "id": "5db0da88a7a1a01c",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:10:01.070059Z",
     "start_time": "2025-05-14T18:10:01.021309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Count the total number of movies.\n",
    "total_movies = movies_rdd.count()\n",
    "total_movies"
   ],
   "id": "8a79ba93e525e108",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1 line = 1 movie\n",
    "\n",
    "Total movies = 5"
   ],
   "id": "e2c572ad739a7634"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:10:01.106295Z",
     "start_time": "2025-05-14T18:10:01.088178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Filter movies with rating >= 4.\n",
    "movies_rated_4_or_plus = (\n",
    "    movies_rdd.filter(lambda fields: float(fields[3]) >= 4.0)\n",
    "    .map(lambda fields: (fields[1], fields[2], float(fields[3])))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "movies_rated_4_or_plus"
   ],
   "id": "3b2ac5644bc63eec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Inception', 'Sci-Fi', 4.8),\n",
       " ('The Dark Knight', 'Action', 4.9),\n",
       " ('The Notebook', 'Romance', 4.2),\n",
       " ('Interstellar', 'Sci-Fi', 4.7)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filter lines with rating >= 4.0 -> We take the second field (movie name), third field (genre) and fourth field (rating).",
   "id": "8a2b53aa4f2f480"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:10:01.142377Z",
     "start_time": "2025-05-14T18:10:01.120662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Display the top 5 movies by rating. (Sort + Limit)\n",
    "top_5_movies = (\n",
    "    movies_rdd.map(lambda fields: (fields[1], fields[2], float(fields[3])))\n",
    "    .sortBy(lambda fields: float(fields[2]), ascending=False)\n",
    "    .take(5)\n",
    ")\n",
    "\n",
    "top_5_movies"
   ],
   "id": "1870513ca63daab9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Dark Knight', 'Action', 4.9),\n",
       " ('Inception', 'Sci-Fi', 4.8),\n",
       " ('Interstellar', 'Sci-Fi', 4.7),\n",
       " ('The Notebook', 'Romance', 4.2),\n",
       " ('Fast & Furious', 'Action', 3.9)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second field (movie name), third field (genre) and fourth field (rating) -> Sort by rating -> Take the top 5 movies.",
   "id": "d06422d9e4268e31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:10:02.144540Z",
     "start_time": "2025-05-14T18:10:01.154684Z"
    }
   },
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "afb2da8fb1627dfb",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 6: Student Scores Analysis\n",
    "Objective: Understand RDD transformations and actions.\n",
    "\n",
    "Tasks:\n",
    "1. Load data using SparkContext.textFile.\n",
    "2. Map each line to a key-value pair (name, score).\n",
    "3. Filter students who scored above 80.\n",
    "4. Count how many students scored above 80."
   ],
   "id": "d20b266fa057123b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:10:02.753982Z",
     "start_time": "2025-05-14T18:10:02.151615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load data using SparkContext.textFile.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-StudentScoresAnalysis\")\n",
    "students_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/6 analysis.csv\")\n",
    "\n",
    "students_rdd.collect()"
   ],
   "id": "5e3fae8389804ee5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Alice', 'Math', '85'],\n",
       " ['Bob', 'Math', '75'],\n",
       " ['Charlie', 'Math', '90'],\n",
       " ['Diana', 'Math', '88'],\n",
       " ['Eve', 'Math', '60']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:10:02.783520Z",
     "start_time": "2025-05-14T18:10:02.767437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Map each line to a key-value pair (name, score).\n",
    "students_scores_rdd = students_rdd.map(lambda fields: (fields[0], float(fields[2])))\n",
    "\n",
    "students_scores_rdd.collect()"
   ],
   "id": "1f779943adfb4ea5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 85.0),\n",
       " ('Bob', 75.0),\n",
       " ('Charlie', 90.0),\n",
       " ('Diana', 88.0),\n",
       " ('Eve', 60.0)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the first field (name) and the third field (score) -> We create a tuple with the name and score.",
   "id": "4d557a3d51767bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:10:02.823479Z",
     "start_time": "2025-05-14T18:10:02.806858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Filter students who scored above 80.\n",
    "students_above_80 = (\n",
    "    students_rdd.filter(lambda fields: float(fields[2]) > 80.0)\n",
    "    .map(lambda fields: (fields[0], fields[1]))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "students_above_80"
   ],
   "id": "bd25a50e0872b9cc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 'Math'), ('Charlie', 'Math'), ('Diana', 'Math')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filter by score > 80.0 (field n°3) -> We take the first field (name) and the second field (subject).",
   "id": "8c3290a5ec0de093"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:10:02.855754Z",
     "start_time": "2025-05-14T18:10:02.838558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Count how many students scored above 80.\n",
    "count_students_above_80 = students_rdd.filter(\n",
    "    lambda fields: float(fields[2]) > 80.0\n",
    ").count()\n",
    "\n",
    "print(count_students_above_80)\n",
    "\n",
    "# or\n",
    "count_students_above_80 = len(students_above_80)\n",
    "count_students_above_80"
   ],
   "id": "fb7d5494743023f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:10:03.858738Z",
     "start_time": "2025-05-14T18:10:02.873891Z"
    }
   },
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "1e782280f7324144",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 7: Word Count from News Articles\n",
    "Objective: Classic Word Count example.\n",
    "\n",
    "Tasks:\n",
    "1. Load the news data file.\n",
    "2. Split lines into words.\n",
    "3. Map each word to (word, 1).\n",
    "4. Use reduceByKey to get word counts"
   ],
   "id": "ae645543e8ce0667"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:10:03.932007Z",
     "start_time": "2025-05-14T18:10:03.866947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load the news data file.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-WordCount\")\n",
    "news_rdd = sc.textFile(f\"{DIR_CONTEXT}/7 wordcount.csv\", 0)\n",
    "\n",
    "news_rdd.collect()"
   ],
   "id": "31f990da0329dc56",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark is fast. Big data is booming. Spark handles big data with ease.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:10:04.454679Z",
     "start_time": "2025-05-14T18:10:03.940731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Split lines into words.\n",
    "words_rdd = news_rdd.flatMap(lambda line: line.replace(\".\", \"\").split(\" \"))\n",
    "\n",
    "words_rdd.collect()"
   ],
   "id": "3c5d3fba22c6dbd7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark',\n",
       " 'is',\n",
       " 'fast',\n",
       " 'Big',\n",
       " 'data',\n",
       " 'is',\n",
       " 'booming',\n",
       " 'Spark',\n",
       " 'handles',\n",
       " 'big',\n",
       " 'data',\n",
       " 'with',\n",
       " 'ease']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split words by space",
   "id": "61018b29ac4cb3ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:10:04.488281Z",
     "start_time": "2025-05-14T18:10:04.471297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Map each word to (word, 1).\n",
    "word_pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "word_pairs_rdd.collect()"
   ],
   "id": "c319efc7747662f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 1),\n",
       " ('is', 1),\n",
       " ('fast', 1),\n",
       " ('Big', 1),\n",
       " ('data', 1),\n",
       " ('is', 1),\n",
       " ('booming', 1),\n",
       " ('Spark', 1),\n",
       " ('handles', 1),\n",
       " ('big', 1),\n",
       " ('data', 1),\n",
       " ('with', 1),\n",
       " ('ease', 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take each word and create a tuple with the word and 1 (1 occurrence).",
   "id": "558bf4911350cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:10:04.550301Z",
     "start_time": "2025-05-14T18:10:04.515065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Use reduceByKey to get word counts\n",
    "word_counts_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "word_counts_rdd.collect()"
   ],
   "id": "fa1ffc9af32ecaa6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 2),\n",
       " ('is', 2),\n",
       " ('fast', 1),\n",
       " ('Big', 1),\n",
       " ('data', 2),\n",
       " ('booming', 1),\n",
       " ('handles', 1),\n",
       " ('big', 1),\n",
       " ('with', 1),\n",
       " ('ease', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-14T18:10:04.562112Z"
    }
   },
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "f2826fd70b101c16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 8: Product Sales Tracker\n",
    "Objective: Group and aggregate product sales.\n",
    "\n",
    "Tasks:\n",
    "1. Load product sales CSV.\n",
    "2. Map to (product, revenue).\n",
    "3. Use reduceByKey to get total revenue.\n",
    "4. Filter products with revenue > 1000."
   ],
   "id": "b3554ac1f38db64b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load product sales CSV.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-ProductSalesTracker\")\n",
    "product_sales_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/8 product.csv\")\n",
    "\n",
    "product_sales_rdd.collect()"
   ],
   "id": "e2ef78c928d8c073",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Map to (product, revenue).\n",
    "product_revenue_rdd = product_sales_rdd.map(\n",
    "    lambda fields: (fields[1], float(fields[2]) * float(fields[3]))\n",
    ")\n",
    "\n",
    "product_revenue_rdd.collect()"
   ],
   "id": "fb5df9acd52fd7f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second field (product) and the third field (revenue) -> We create a tuple with the product and revenue.",
   "id": "106c2da76ba22d6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Use reduceByKey to get total revenue.\n",
    "total_revenue_rdd = product_revenue_rdd.reduceByKey(lambda a, b: a + b)  # ??\n",
    "\n",
    "total_revenue_rdd.collect()\n",
    "\n",
    "# Getwhole total revenue\n",
    "total_revenue = total_revenue_rdd.map(lambda fields: fields[1]).sum()\n",
    "\n",
    "total_revenue"
   ],
   "id": "d8f3e55add70aeae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this case, the usage of reduceByKey is not necessary since there's no duplicate keys.\n",
    "\n",
    "To get the total revenue, we can simply sum values."
   ],
   "id": "6d856487fd289e41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Filter products with revenue > 1000.\n",
    "filtered_revenue_rdd = total_revenue_rdd.filter(lambda fields: fields[1] > 1000)\n",
    "\n",
    "filtered_revenue_rdd.collect()"
   ],
   "id": "c6b0796c461b0b0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filter the products with revenue > 1000.",
   "id": "78055f9e7091357d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "8ce4955c21814355",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 9: Temperature Data Monitoring\n",
    "Objective: Work with sensor data and aggregate.\n",
    "\n",
    "Tasks:\n",
    "1. Load temperature log.\n",
    "2. Extract date and temperature.\n",
    "3. Calculate daily average temperature.\n",
    "4. Filter days with average temperature > 30°C."
   ],
   "id": "adad2448fcf7e44d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load temperature log.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-TemperatureDataMonitoring\")\n",
    "temperature_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/9 temp.csv\")\n",
    "\n",
    "temperature_rdd.collect()"
   ],
   "id": "609577da92635523",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Extract date and temperature.\n",
    "temperature_data_rdd = temperature_rdd.map(lambda fields: (fields[0], float(fields[2])))\n",
    "\n",
    "temperature_data_rdd.collect()"
   ],
   "id": "54c9d7616980ff6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the first field (date) and the third field (temperature) -> We create a tuple with the date and temperature.",
   "id": "8b7cff052ec82186"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Calculate daily average temperature.\n",
    "daily_avg_temp_rdd = temperature_data_rdd.groupByKey().mapValues(\n",
    "    lambda temps: sum(temps) / len(temps)\n",
    ")\n",
    "\n",
    "daily_avg_temp_rdd.collect()"
   ],
   "id": "269171983cc220c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We use groupByKey to group by date -> We calculate the average temperature for each date.",
   "id": "4717fffb8453e38e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Filter days with average temperature > 30°C.\n",
    "filtered_temp_rdd = daily_avg_temp_rdd.filter(lambda fields: fields[1] > 30)\n",
    "\n",
    "filtered_temp_rdd.collect()"
   ],
   "id": "4a73e6b9b96e1321",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the average temperature (produced previously) and filter the days with average temperature > 30°C.",
   "id": "f18af86ccf1e44ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "bf2bea5b4c86e465",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 10: Customer Feedback Sentiment\n",
    "Objective: Perform simple NLP with Spark.\n",
    "\n",
    "Tasks:\n",
    "1. Load feedback text file.\n",
    "2. Clean the text (lowercase, remove punctuation).\n",
    "3. Tokenize and count positive/negative words.\n",
    "4. Classify as positive/negative based on score."
   ],
   "id": "f615f656090faa40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load feedback text file.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-CustomerFeedbackSentiment\")\n",
    "feedback_rdd = sc.textFile(f\"{DIR_CONTEXT}/10 feedback.csv\")\n",
    "\n",
    "feedback_rdd.collect()"
   ],
   "id": "c7824aff9abe30ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Clean the text (lowercase, remove punctuation).\n",
    "\n",
    "clean_text = feedback_rdd.map(lambda line: line.lower().replace(\".\", \"\"))\n",
    "\n",
    "clean_text.collect()"
   ],
   "id": "f4c21d44ff1f930a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here, punctuation is only composed of dots. We simply use lower() function to lowercase text and replace dots with empty string.\n",
    "\n",
    "Result: ['the service was excellent and food was amazing',\n",
    " 'terrible experience the wait time was long',\n",
    " 'loved the ambiance and friendly staff']"
   ],
   "id": "4753a9ac54d3a71a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Tokenize and count positive/negative words.\n",
    "positive_words = [\"excellent\", \"amazing\", \"loved\", \"friendly\"]\n",
    "negative_words = [\"terrible\", \"long\"]\n",
    "\n",
    "\n",
    "def count_sentiment_words(line) -> (int, int):\n",
    "    words = line.split()\n",
    "    positive_count = len([1 for word in words if word in positive_words])\n",
    "    negative_count = len([1 for word in words if word in negative_words])\n",
    "    return positive_count, negative_count\n",
    "\n",
    "\n",
    "sentiment_counts = clean_text.map(count_sentiment_words)\n",
    "\n",
    "sentiment_counts.collect()"
   ],
   "id": "3b55faf473077dce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Asked to tokenize to count positive and negative words. <br>\n",
    "We split the line into words -> We count the number of positive and negative words (defined above). <br>\n",
    "We map the clean text to the count_sentiment_words function.\n",
    "\n",
    "Result:\n",
    "- 1st line: 2 positive, 0 negative\n",
    "- 2nd line: 0 positive, 2 negative\n",
    "- 3rd line: 2 positive, 0 negative"
   ],
   "id": "ab641e9a2854e7da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Classify as positive/negative based on score.\n",
    "\n",
    "\n",
    "def classify_sentiment(counts) -> str:\n",
    "    positive_count, negative_count = counts\n",
    "    if positive_count > negative_count:\n",
    "        return \"Positive\"\n",
    "    elif positive_count < negative_count:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "\n",
    "sentiment_classification = sentiment_counts.map(classify_sentiment)\n",
    "\n",
    "sentiment_classification.collect()"
   ],
   "id": "6411a249ba7aa733",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We fetch the positive and negative counts -> We classify as positive, negative or neutral based on the counts.\n",
    "\n",
    "Result:\n",
    "- 1st line: Positive\n",
    "- 2nd line: Negative\n",
    "- 3rd line: Positive"
   ],
   "id": "96536da1aed5dbd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "6bc606285145a90e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 11: Email Logs Analyzer\n",
    "Objective: Analyze organizational email traffic.\n",
    "\n",
    "Tasks:\n",
    "1. Load logs.\n",
    "2. Map each line to (sender, 1).\n",
    "3. Count emails sent by each user.\n",
    "4. Identify the top 3 senders"
   ],
   "id": "994d801fc1a13d89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load logs.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-EmailLogsAnalyzer\")\n",
    "email_logs_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/11 email.csv\")\n",
    "\n",
    "email_logs_rdd.collect()"
   ],
   "id": "82cad87ee154bbdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Map each line to (sender, 1).\n",
    "email_senders_rdd = email_logs_rdd.map(lambda fields: (fields[1], 1))\n",
    "\n",
    "email_senders_rdd.collect()"
   ],
   "id": "8cac6dd9a6bd401e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sender is the second field (index 1). We create a tuple with the sender and 1 (1 email sent).\n",
    "\n",
    "Result: [('john@abc.com', 1), ('john@abc.com', 1), ('amy@abc.com', 1)]"
   ],
   "id": "4d5a81bc74e020ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Count emails sent by each user.\n",
    "\n",
    "email_counts_rdd = email_senders_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "email_counts_rdd.collect()"
   ],
   "id": "52c69b737ca3b0b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We reduce by key (sender) to count the number of emails sent by each user.\n",
    "\n",
    "Result: [('john@abc.com', 2), ('amy@abc.com', 1)]"
   ],
   "id": "26a2f3f431503dc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Identify the top 3 senders\n",
    "\n",
    "top_3_senders = email_counts_rdd.sortBy(lambda lines: lines[1], ascending=False).take(3)\n",
    "\n",
    "top_3_senders"
   ],
   "id": "f1fbbf2940367c95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We sort by the number of emails sent (index 1) from email counts rdd and take the top 3 senders.\n",
    "\n",
    "Result: [('john@abc.com', 2), ('amy@abc.com', 1)]\n",
    "\n",
    "(Result would be more relevant with a bigger dataset)"
   ],
   "id": "2022e52b478277d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "2d6ed5b1d96e8311",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 12: Online Orders Processing\n",
    "Objective: Perform filtering and grouping.\n",
    "\n",
    "Tasks:\n",
    "1. Load orders.\n",
    "2. Filter orders with status = “Shipped”.\n",
    "3. Group orders by region.\n",
    "4. Count orders per region."
   ],
   "id": "827734707ce41006"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load orders.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-OnlineOrdersProcessing\")\n",
    "orders_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/12 orders.csv\")\n",
    "\n",
    "orders_rdd.collect()"
   ],
   "id": "590c04aedb56e9bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Filter orders with status = “Shipped”.\n",
    "shipped_orders_rdd = orders_rdd.filter(lambda fields: fields[2] == \"Shipped\")\n",
    "\n",
    "shipped_orders_rdd.collect()"
   ],
   "id": "a5dd2eeef0ac392a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filter the orders with status = \"Shipped\" (index 2).",
   "id": "29149778231c09b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Group orders by region.\n",
    "grouped_orders_rdd = (\n",
    "    shipped_orders_rdd.map(lambda fields: (fields[1], fields[0]))\n",
    "    .groupByKey()\n",
    "    .map(lambda fields: (fields[0], list(fields[1])))\n",
    ")\n",
    "\n",
    "grouped_orders_rdd.collect()"
   ],
   "id": "34493e89df435800",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second field (region) and the first field (order ID) -> Group by region -> Create a tuple with the region and the list of order IDs.",
   "id": "df007b10d4b22679"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Count orders per region.\n",
    "orders_count_per_region = grouped_orders_rdd.map(\n",
    "    lambda fields: (fields[0], len(fields[1]))\n",
    ")\n",
    "\n",
    "orders_count_per_region.collect()"
   ],
   "id": "86b0d677c8054f38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the rdd from the previous step and count the number of orders per region (length of the list of order IDs).",
   "id": "e4bb6497127bce90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "c95815979b6b7ee8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 13: IoT Data Aggregation\n",
    "Objective: Work with streaming-like IoT logs.\n",
    "\n",
    "Tasks:\n",
    "1. Load device logs.\n",
    "2. Filter faulty sensor readings (<0).\n",
    "3. Count faults by sensor.\n",
    "4. Plot data (use Python later)"
   ],
   "id": "afdad35ac19662d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load device logs.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-IoTDataAggregation\")\n",
    "device_logs_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/13 iot.csv\")\n",
    "\n",
    "device_logs_rdd.collect()"
   ],
   "id": "ab29d2c23343139a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Filter faulty sensor readings (<0).\n",
    "faulty_sensor_readings_rdd = device_logs_rdd.filter(lambda fields: float(fields[2]) < 0)\n",
    "\n",
    "faulty_sensor_readings_rdd.collect()"
   ],
   "id": "8c1ae59be6d74b97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filter every faulty sensor (index 2) with a value < 0.",
   "id": "a054ac30eee97a3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Count faults by sensor.\n",
    "faults_by_sensor_rdd = faulty_sensor_readings_rdd.map(\n",
    "    lambda fields: (fields[1], 1)\n",
    ").reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "faults_by_sensor_rdd.collect()"
   ],
   "id": "63c4d7d48e374249",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take rdd from the previous step and create a tuple with the sensor ID (index 1) and 1 cpt (for 1 fault）-> Reduce by key to count the number of faults per sensor.",
   "id": "79fd846ee5a085f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Plot data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sensors = [x[0] for x in faults_by_sensor_rdd.collect()]\n",
    "fault_counts = [x[1] for x in faults_by_sensor_rdd.collect()]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(sensors, fault_counts)\n",
    "plt.xlabel(\"Sensor ID\")\n",
    "plt.ylabel(\"Number of Faults\")\n",
    "plt.title(\"Faults per Sensor\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "93541f446cf2ba46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since there is no indication on ploting, we will arbitrarily choose to plot the number of faults per sensor.\n",
    "We take the sensor ID and the number of faults -> We plot a bar chart with the sensor ID on the x-axis and the number of faults on the y-axis."
   ],
   "id": "e98ffccbffcd271b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "924a1042bb3da288",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 14: Bank Transactions Fraud Detection\n",
    "Objective: Find suspicious transactions.\n",
    "\n",
    "Tasks:\n",
    "1. Load transactions.\n",
    "2. Filter transactions over $10,000.\n",
    "3. Group by account.\n",
    "4. Identify accounts with multiple high-value transactions."
   ],
   "id": "2183b0ccac381361"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load transactions.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-BankTransactionsFraudDetection\")\n",
    "transactions_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/14 bank.csv\")\n",
    "\n",
    "transactions_rdd.collect()"
   ],
   "id": "cf986a6da85c4662",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Filter transactions over $10,000.\n",
    "high_value_transactions_rdd = transactions_rdd.filter(\n",
    "    lambda fields: float(fields[2]) > 10000\n",
    ")\n",
    "\n",
    "high_value_transactions_rdd.collect()"
   ],
   "id": "7dd6e9b2a68a77ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filter the transactions with a value > 10000 (index 2).",
   "id": "8b357610ad734a06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Group by account.\n",
    "group_byaccount_rdd = (\n",
    "    high_value_transactions_rdd.map(lambda fields: (fields[1], fields[0]))\n",
    "    .groupByKey()\n",
    "    .map(lambda fields: (fields[0], list(fields[1])))\n",
    ")\n",
    "\n",
    "group_byaccount_rdd.collect()"
   ],
   "id": "aa7affeab3221167",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second field (account) and the first field (transaction ID) -> Group by account -> Create a tuple with the account and the list of transaction IDs.",
   "id": "95bc6893022ddda6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Identify accounts with multiple high-value transactions.\n",
    "multiple_high_value_accounts_rdd = group_byaccount_rdd.filter(\n",
    "    lambda fields: len(fields[1]) > 1\n",
    ")\n",
    "\n",
    "multiple_high_value_accounts_rdd.collect()"
   ],
   "id": "ace8a41390697fc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From previous step, we filter the accounts with more than 1 transaction ID (multiple high-value transactions).",
   "id": "ee6a0d88b502e762"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "551ce1ee4ee27ce5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 15: E-commerce Product Clicks\n",
    "Objective: Perform session-based analysis.\n",
    "\n",
    "Tasks:\n",
    "1. Load click logs.\n",
    "2. Group clicks by session ID.\n",
    "3. Count number of clicks per session.\n",
    "4. Find average clicks per session"
   ],
   "id": "a1e19733a63bf045"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load click logs.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-EcommerceProductClicks\")\n",
    "click_logs_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/15 clicks.csv\")\n",
    "\n",
    "click_logs_rdd.collect()"
   ],
   "id": "d02541de754a92e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Group clicks by session ID.\n",
    "group_by_session_rdd = (\n",
    "    click_logs_rdd.map(lambda fields: (fields[0], fields[1]))\n",
    "    .groupByKey()\n",
    "    .map(lambda fields: (fields[0], list(fields[1])))\n",
    ")\n",
    "\n",
    "group_by_session_rdd.collect()"
   ],
   "id": "d226a4ae02e6c8bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the first field (session ID) and the second field (product ID) -> Group by session ID -> Create a tuple with the session ID and the list of product IDs.",
   "id": "58de55c9c1cc6da2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Count number of clicks per session.\n",
    "clicks_per_session_rdd = group_by_session_rdd.map(\n",
    "    lambda fields: (fields[0], len(fields[1]))\n",
    ")\n",
    "\n",
    "clicks_per_session_rdd.collect()"
   ],
   "id": "cf017cd77d0bc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From previous step, we take the session ID and count the number of product IDs (clicks) per session.",
   "id": "e5265dc0361f2c21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Find average clicks per session.\n",
    "average_clicks_per_session = clicks_per_session_rdd.map(lambda fields: fields[1]).mean()\n",
    "\n",
    "average_clicks_per_session"
   ],
   "id": "deeef92c19cb783",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From previous step, we take the number of clicks and calculate the average with mean().",
   "id": "c9cb0721ced760a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "3867f8bfb7884483",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 16: Exercise: Delivery Time Performance\n",
    "Objective: Measure delivery speed.\n",
    "\n",
    "Tasks:\n",
    "1. Load orders.\n",
    "2. Calculate delivery time = Delivered - Ordered.\n",
    "3. Average delivery time per region.\n",
    "4. Identify delays > average"
   ],
   "id": "55d545c31a97798d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load orders.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-DeliveryTimePerformance\")\n",
    "orders_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/16 delivery.csv\")\n",
    "\n",
    "orders_rdd.collect()"
   ],
   "id": "7c7ba9325255ba4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Calculate delivery time = Delivered - Ordered.\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def parse_dates(fields) -> (str, int):\n",
    "    region = fields[1]\n",
    "    ordered_date = datetime.strptime(fields[2], \"%Y-%m-%d\")\n",
    "    delivered_date = datetime.strptime(fields[3], \"%Y-%m-%d\")\n",
    "    delivered_time = (delivered_date - ordered_date).days\n",
    "    return region, delivered_time\n",
    "\n",
    "\n",
    "delivery_time_rdd = orders_rdd.map(parse_dates)\n",
    "\n",
    "delivery_time_rdd.collect()"
   ],
   "id": "2c412da1659839e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We create a function that will\n",
    "1. Take the region (index 1)\n",
    "2. Parse the ordered date (index 2) and delivered date (index 3) to datetime objects -> Create delivery time (in days)\n",
    "3. Return a tuple with the region and the delivery time\n",
    "\n",
    "We apply the function to the orders rdd."
   ],
   "id": "be442d48320f2fa7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Average delivery time per region.\n",
    "average_delivery_time_rdd = delivery_time_rdd.groupByKey().mapValues(\n",
    "    lambda times: sum(times) / len(times)\n",
    ")\n",
    "\n",
    "average_delivery_time_rdd.collect()"
   ],
   "id": "5ae4b044f9163fbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since we have the region and the delivery time, we group by region -> Calculate the average delivery time per region. <br>\n",
    "There is no relevant information since there's only one order by region in the dataset.\n",
    "To get a more relevant result, we could add a new line for a specific region with a higher delivery time to see if the average changes.\n",
    "\n",
    "```csv\n",
    "OrderID,Region,OrderedDate,DeliveredDate\n",
    "1,North,2025-05-01,2025-05-03\n",
    "2,South,2025-05-01,2025-05-02\n",
    "3,East,2025-05-01,2025-05-05\n",
    "4,East,2025-05-01,2025-05-30\n",
    "```\n",
    "```bash\n",
    ">>> [('North', 2.0), ('South', 1.0), ('East', 16.5)]\n",
    "```"
   ],
   "id": "c370cf7d4808418"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Identify delays > average\n",
    "average_delivery_time_dict = average_delivery_time_rdd.collectAsMap()\n",
    "delays = delivery_time_rdd.filter(\n",
    "    lambda field: field[1] > average_delivery_time_dict[field[0]]\n",
    ").map(lambda field: (field[0], field[1]))\n",
    "\n",
    "delays.collect()"
   ],
   "id": "2105fa2b868edd7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We convert the average delivery time rdd to a dictionary -> We filter the delivery time rdd to get the delays (delivery time > average delivery time) -> We create a tuple with all information about it.",
   "id": "4729cab69f0daef3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "c991b694d5b3deca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 17: Web Logs Traffic Summary\n",
    "Objective: Analyze visits per IP.\n",
    "\n",
    "Tasks:\n",
    "1. Load access logs.\n",
    "2. Map to (IP, 1).\n",
    "3. Count visits by IP.\n",
    "4. Find IPs with > 3 visits."
   ],
   "id": "f10b6de96b69bbdb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load access logs.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-WebLogsTrafficSummary\")\n",
    "access_logs_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/17 accesslog.csv\")\n",
    "\n",
    "access_logs_rdd.collect()"
   ],
   "id": "290908a8d150930c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Map to (IP, 1).\n",
    "ip_visits_rdd = access_logs_rdd.map(lambda fields: (fields[0], 1))\n",
    "\n",
    "ip_visits_rdd.collect()"
   ],
   "id": "b19bce874ca2b3e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Count visits by IP.\n",
    "ip_counts_rdd = ip_visits_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "ip_counts_rdd.collect()"
   ],
   "id": "128b09d323a70ae0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From last step, we reduce by key (IP) to count the number of visits by IP.",
   "id": "f2383b4244c099e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Find IPs with > 3 visits.\n",
    "ips_with_more_than_3_visits = ip_counts_rdd.filter(lambda fields: fields[1] > 3)\n",
    "\n",
    "ips_with_more_than_3_visits.collect()"
   ],
   "id": "c0ba96223d9a1fb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From last step, we filter the IPs with more than 3 visits. <br>\n",
    "Since there's no IP with more than 3 visits in the dataset, we get an empty list."
   ],
   "id": "9b9189a5441cf759"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "98248a76077171c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 18: Customer Lifetime Value\n",
    "Objective: Analyze total customer revenue.\n",
    "\n",
    "Tasks:\n",
    "1. Load transactions.\n",
    "2. Group by customer.\n",
    "3. Sum total spending.\n",
    "4. Rank customers by spending."
   ],
   "id": "e9deb89544050417"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load transactions.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-CustomerLifetimeValue\")\n",
    "transactions_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/18 customers.csv\")\n",
    "\n",
    "transactions_rdd.collect()"
   ],
   "id": "e7e201d2f03fbe01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Group by customer.\n",
    "group_by_customer_rdd = (\n",
    "    transactions_rdd.map(lambda fields: (fields[0], float(fields[1])))\n",
    "    .groupByKey()\n",
    "    .map(lambda fields: (fields[0], list(fields[1])))\n",
    ")\n",
    "\n",
    "group_by_customer_rdd.collect()"
   ],
   "id": "ba5b09849ffbca4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Sum total spending.\n",
    "total_spending_rdd = group_by_customer_rdd.map(\n",
    "    lambda fields: (fields[0], sum(fields[1]))\n",
    ")\n",
    "\n",
    "total_spending_rdd.collect()"
   ],
   "id": "e6ff5c737e9569a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For each customer, we take the first field (customer ID) and the second field (spending) -> We group by customer -> We create a tuple with the customer ID and the list of spending -> We sum the spending for each customer.",
   "id": "16e27f542ee2595b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Rank customers by spending.\n",
    "ranked_customers_rdd = total_spending_rdd.sortBy(\n",
    "    lambda fields: fields[1], ascending=False\n",
    ")\n",
    "\n",
    "ranked_customers_rdd.collect()"
   ],
   "id": "199976d31ea11d4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From last step, we sort by spending (index 1) to rank the customers by spending.",
   "id": "3ce9eaaedfdaa53c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:05:34.705263Z",
     "start_time": "2025-05-14T18:05:34.631824Z"
    }
   },
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "7b4aa2828713b77c",
   "outputs": [],
   "execution_count": 186
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 19: Covid Case Trend Analysis\n",
    "Objective: Time-series aggregation.\n",
    "\n",
    "Tasks:\n",
    "1. Load covid daily data.\n",
    "2. Group by date.\n",
    "3. Sum total cases.\n",
    "4. Compute daily growth."
   ],
   "id": "763d6dc0045df610"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:05:36.342515Z",
     "start_time": "2025-05-14T18:05:35.760181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load covid daily data.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-CovidCaseTrendAnalysis\")\n",
    "covid_data_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/covid.csv\")\n",
    "\n",
    "covid_data_rdd.collect()"
   ],
   "id": "c9c6c151a38d1ef7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2025-05-01', 'NY', '100'],\n",
       " ['2025-05-01', 'CA', '120'],\n",
       " ['2025-05-02', 'NY', '150'],\n",
       " ['2025-05-02', 'CA', '130']]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 187
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:05:59.033044Z",
     "start_time": "2025-05-14T18:05:58.973383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Group by date. Sum total cases.\n",
    "daily_cases = (\n",
    "    covid_data_rdd.map(lambda x: (x[0], int(x[2])))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .sortByKey()\n",
    ")\n",
    "\n",
    "daily_cases.collect()"
   ],
   "id": "cbe764b465a496e1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2025-05-01', 220), ('2025-05-02', 280)]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 190
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:07:00.284261Z",
     "start_time": "2025-05-14T18:07:00.271606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Compute daily growth.\n",
    "\n",
    "daily_sorted = daily_cases.collect()\n",
    "growth = [\n",
    "    (daily_sorted[i][0], daily_sorted[i][1] - daily_sorted[i - 1][1])\n",
    "    for i in range(1, len(daily_sorted))\n",
    "]\n",
    "\n",
    "print(\"Daily Totals:\", daily_sorted)\n",
    "print(\"Daily Growth:\", growth)"
   ],
   "id": "906de0cd719e73f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Totals: [('2025-05-01', 220), ('2025-05-02', 280)]\n",
      "Daily Growth: [('2025-05-02', 60)]\n"
     ]
    }
   ],
   "execution_count": 198
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We take the first field (date) and the third field (cases) -> We reduce by key (date) to sum the total cases per date -> We sort by date. <br>\n",
    "We create a list with the date and the growth (current day - previous day) -> We print the daily totals and the daily growth."
   ],
   "id": "c7bca078668265de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:08:03.486465Z",
     "start_time": "2025-05-14T18:08:03.079104Z"
    }
   },
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "676ed3f86af9e6bd",
   "outputs": [],
   "execution_count": 200
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 20: Real-Time Chat Logs Analysis\n",
    "Objective: Analyze chat frequency per user.\n",
    "\n",
    "Tasks:\n",
    "1. Load chat logs.\n",
    "2. Count messages per user.\n",
    "3. Identify top 3 most active users.\n",
    "4. Filter users with < 2 messages"
   ],
   "id": "5097f93e71c0bd7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:08:04.596339Z",
     "start_time": "2025-05-14T18:08:04.016989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load chat logs.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-RealTimeChatLogsAnalysis\")\n",
    "chat_logs_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/20 chatlog.csv\")\n",
    "\n",
    "chat_logs_rdd.collect()"
   ],
   "id": "de93f382f36ea132",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Alice', 'Hello!'],\n",
       " ['Bob', 'Hi!'],\n",
       " ['Alice', 'How are you?'],\n",
       " ['Charlie', 'Hey!'],\n",
       " ['Alice', 'Welcome!']]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 201
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:08:43.141535Z",
     "start_time": "2025-05-14T18:08:43.113760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Count messages per user.\n",
    "user_messages_rdd = chat_logs_rdd.map(lambda fields: (fields[0], 1)).reduceByKey(\n",
    "    lambda a, b: a + b\n",
    ")\n",
    "\n",
    "user_messages_rdd.collect()"
   ],
   "id": "8981ca49ed9c5244",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 3), ('Bob', 1), ('Charlie', 1)]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 202
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Identify top 3 most active users.\n",
    "\n",
    "top_3_active_users = user_messages_rdd.sortBy(lambda fields: fields[1], ascending=False).take(\n",
    "    3\n",
    ")\n",
    "\n",
    "top_3_active_users"
   ],
   "id": "e732585003b04bba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Result is not relevant since there are only 3 users in the dataset.",
   "id": "f3b629a6582bbfa7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:09:39.610062Z",
     "start_time": "2025-05-14T18:09:39.591705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Filter users with < 2 messages\n",
    "less_than_2_messages = user_messages_rdd.filter(lambda fields: fields[1] < 2)\n",
    "\n",
    "less_than_2_messages.collect()"
   ],
   "id": "24eb9260077c7e47",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bob', 1), ('Charlie', 1)]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 203
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sc.stop()",
   "id": "cdcc1b774acb3bc1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
