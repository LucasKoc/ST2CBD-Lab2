{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T06:41:21.218942Z",
     "start_time": "2025-05-14T06:41:21.144016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pip install pyspark\n",
    "from pyspark import SparkContext, RDD\n",
    "\n",
    "DIR_PATH = \"Sparks_dataset/Sparks_dataset\"\n",
    "DIR_CONTEXT = DIR_PATH + \"/Context\"\n",
    "DIR_DATA_FRAME = DIR_PATH + \"/Data Frame\"\n",
    "DIR_SPARK_PANDAS = DIR_PATH + \"/Spark Pandas\""
   ],
   "id": "95868fa35c1cb0a2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 1 - Basic Order Analysis\n",
    "Tasks:\n",
    "\n",
    "1. Load the dataset into Spark using SparkContext and textFile().\n",
    "2. Count the total number of orders.\n",
    "3. Calculate the total revenue from all orders.\n",
    "4. Find the number of unique customers."
   ],
   "id": "e67fddf6f9076e31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load the dataset into Spark using SparkContext and textFile().\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-OrderAnalysis\")\n",
    "\n",
    "# Load the dataset\n",
    "salesdata_rdd = sc.textFile(f\"{DIR_CONTEXT}/salesdata.csv.txt\")"
   ],
   "id": "c7cc47fcb82bc656",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Count the total number of orders.\n",
    "# Because the file has 2 header lines, we need to remove them before counting\n",
    "header = salesdata_rdd.take(2)\n",
    "\n",
    "# We remove the header lines from the RDD\n",
    "salesdata_rdd = salesdata_rdd.filter(lambda line: line != header[0] and line != header[1])\n",
    "\n",
    "# Count the total number of orders\n",
    "total_orders = salesdata_rdd.count()\n",
    "\n",
    "total_orders"
   ],
   "id": "9ac53eae98673040",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the CSV file, we have 2 header lines. In order to count the total number of orders, we need to remove the header lines from the RDD.\n",
    "\n",
    "Total number of orders: 10"
   ],
   "id": "38210f1a950be4b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Calculate the total revenue from all orders.\n",
    "# Index of OrderAmount is 2\n",
    "\n",
    "sales_revenues = salesdata_rdd.map(lambda line: float(line.split(\",\")[2]))\n",
    "\n",
    "total_revenue = sales_revenues.sum()\n",
    "\n",
    "total_orders"
   ],
   "id": "784d9bebfe2e1687",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Total orders = 10",
   "id": "17decde7fab0af08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Find the number of unique customers.\n",
    "# Index of CustomerID is 1\n",
    "\n",
    "unique_customers = salesdata_rdd.map(lambda line: line.split(\",\")[1]).distinct()\n",
    "\n",
    "total_unique_customers = unique_customers.count()\n",
    "\n",
    "total_unique_customers"
   ],
   "id": "39cff80a5ef77c38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Total unique customers = 6",
   "id": "c95df7006aa57db5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "7d73cb6bedac9eae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 2 - Department Salary Stats\n",
    "Tasks:\n",
    "\n",
    "1. Load employee data and skip header.\n",
    "2. Find total number of employees.\n",
    "3. Calculate average salary per department.\n",
    "4. Find department with highest total salary."
   ],
   "id": "b19bd42522625867"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T06:41:25.958374Z",
     "start_time": "2025-05-14T06:41:23.445696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load employee data and skip header.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-DepartmentSalaryStats\")\n",
    "\n",
    "# Load the dataset\n",
    "salesdata_rdd = sc.textFile(f\"{DIR_CONTEXT}/2 salary.csv\")\n",
    "\n",
    "# Skip header\n",
    "header = salesdata_rdd.take(1)\n",
    "\n",
    "salesdata_rdd = salesdata_rdd.filter(lambda line: line != header[0])\n",
    "\n",
    "salesdata_rdd.take(1)"
   ],
   "id": "642fe0e0bbca8d2d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/14 08:41:24 WARN Utils: Your hostname, MacBook-Pro-de-Lucas.local resolves to a loopback address: 127.0.0.1; using 10.101.8.226 instead (on interface en0)\n",
      "25/05/14 08:41:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/14 08:41:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"'E101,Alice,Sales,50000']\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Result should not be the header and be the first line: ['E101,Alice,Sales,50000']",
   "id": "8fbff70b1ebdcc47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T06:41:26.051730Z",
     "start_time": "2025-05-14T06:41:25.969771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Find total number of employees.\n",
    "\n",
    "total_employees = salesdata_rdd.count()\n",
    "\n",
    "total_employees"
   ],
   "id": "3f085dea5d24e15b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Total employees = 8",
   "id": "c4a430c554aafb58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T06:41:33.453551Z",
     "start_time": "2025-05-14T06:41:33.244624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Calculate average salary per department.\n",
    "\n",
    "average_salary_per_department = salesdata_rdd.map(lambda line: (line.split(\",\")[2], float(line.split(\",\")[3]))) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(lambda salaries: sum(salaries) / len(salaries)).collect()\n",
    "\n",
    "average_salary_per_department"
   ],
   "id": "b9da783b4208aa85",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sales', 54000.0), ('Marketing', 56500.0), ('IT', 69000.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Departements average salaries:\n",
    "- Sales: 54000\n",
    "- Marketing: 56500\n",
    "- IT: 69000"
   ],
   "id": "b0b687e689119cdf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Find department with the highest total salary.\n",
    "\n",
    "highest_salary_department = salesdata_rdd.map(lambda line: (line.split(\",\")[2], float(line.split(\",\")[3]))) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(lambda salaries: sum(salaries)) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False) \\\n",
    "    .first()\n",
    "\n",
    "highest_salary_department"
   ],
   "id": "9680d37d4acf0c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "f46f6f7354a910e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 3: Web Traffic Summary\n",
    "Dataset: web_logs.txt\n",
    "\n",
    "Tasks:\n",
    "1. Load the data and parse it into fields.\n",
    "2. Count total number of visits per URL.\n",
    "3. Find unique users per URL.\n",
    "4. Determine peak hour for traffic."
   ],
   "id": "ec1ee1b50fe04c62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load the data and parse it into fields.\n",
    "\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-WebTrafficSummary\")\n",
    "web_logs_rdd = sc.textFile(f\"{DIR_CONTEXT}/3 web_logs.txt\")\n",
    "\n",
    "# Skip header\n",
    "header = web_logs_rdd.take(1)\n",
    "web_logs_rdd = web_logs_rdd.filter(lambda line: line != header[0])\n",
    "\n",
    "# Parse the data into fields\n",
    "web_logs_rdd = web_logs_rdd.map(lambda line: line.split(\",\"))"
   ],
   "id": "f77147a349ba9b15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Count total number of visits per URL.\n",
    "\n",
    "url_visits = web_logs_rdd.map(lambda fields: (fields[1], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "\n",
    "url_visits"
   ],
   "id": "8bd797e256d7ee7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We take the URL and the number of visits -> Reduce by key to count visits (line = 1 visit).\n",
    "\n",
    "Result: [('/home', 6), ('/product', 2), ('/contact', 1), ('/about', 1)]"
   ],
   "id": "56d72e94d4641f4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Find unique users per URL.\n",
    "\n",
    "unique_users_per_url = web_logs_rdd.map(lambda fields: (fields[1], fields[0])) \\\n",
    "    .distinct() \\\n",
    "    .map(lambda x: (x[0], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "\n",
    "unique_users_per_url"
   ],
   "id": "2f771cdc6c69e25a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First we take the URL and the user ID -> Remove duplicates -> Reduce by key to count unique users.\n",
    "\n",
    "Result: [('/home', 4), ('/product', 2), ('/contact', 1), ('/about', 1)]"
   ],
   "id": "62cc57ccbe4724df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Determine peak hour for traffic.\n",
    "from datetime import datetime\n",
    "\n",
    "hour_counts = web_logs_rdd.map(lambda fields: (datetime.strptime(fields[2], \"%Y-%m-%d %H:%M:%S\").hour, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "peak_hour = hour_counts.sortBy(lambda x: x[1], ascending=False).first()\n",
    "\n",
    "peak_hour"
   ],
   "id": "a163fa9c32bd4901",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We take the field index 2 (Timestamp) and convert it to a datetime object -> Get the hour -> Reduce by key to count visits per hour.\n",
    "\n",
    "After that we sort by the number of visits and take the first one.\n",
    "\n",
    "Result: (13, 2) -> 13:00 w/ 2 visits"
   ],
   "id": "ebc35afcb94efa6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "65d89b72a36abf1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 4: Product Rating Analysis\n",
    "Tasks: Dataset: product_reviews.csv\n",
    "1. Load and parse the review data.\n",
    "2. Calculate the average rating per product.\n",
    "3. Count number of reviews per product.\n",
    "4. Find products with all 5-star reviews."
   ],
   "id": "13af6222702d4f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_rdd_csv(file_path, header_lines=1) -> RDD[str]:\n",
    "    rdd = sc.textFile(file_path)\n",
    "    header = rdd.take(header_lines)\n",
    "    rdd = rdd.filter(lambda line: line not in header)\n",
    "\n",
    "    rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "    return rdd"
   ],
   "id": "2e6e30bb88d06db7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Created a function for parsing input files. It takes the file path and the number of header lines to skip (header). It returns an RDD with the parsed data.",
   "id": "e803f9cf3dff6b09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load and parse the review data.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-ProductRatingAnalysis\")\n",
    "production_reviews_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/4 product_reviews.csv\")"
   ],
   "id": "998809da18d62a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Calculate the average rating per product\n",
    "average_rating_per_product = production_reviews_rdd.map(lambda fields: (fields[1], float(fields[2]))) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(lambda ratings: sum(ratings) / len(ratings)) \\\n",
    "    .collect()\n",
    "\n",
    "average_rating_per_product"
   ],
   "id": "9af4114cafd5bae1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second (product) and third field (score) -> Group by product -> Calculate the average score.",
   "id": "c43e3be22881a552"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Count number of reviews per product\n",
    "\n",
    "reviews_per_product = production_reviews_rdd.map(lambda fields: (fields[1], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "\n",
    "reviews_per_product"
   ],
   "id": "561de79b9bb419df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second field (product) and count the number of reviews (each line correspond to one product review).",
   "id": "616fac317d4bb247"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Find products with all 5-star reviews.\n",
    "\n",
    "five_star_products = production_reviews_rdd.map(lambda fields: (fields[1], float(fields[2]))) \\\n",
    "    .groupByKey() \\\n",
    "    .filter(lambda x: all(rating == 5.0 for rating in x[1])) \\\n",
    "    .map(lambda x: x[0]) \\\n",
    "    .collect()\n",
    "\n",
    "five_star_products"
   ],
   "id": "3728b02abfb3fc69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second field (product) and the third field (score) -> Group by product -> Filter products with all 5-star reviews",
   "id": "3e8db215417d07a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "f6f5230f0e256195",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 5: Movie Ratings Analytics\n",
    "Objective: Learn to read a CSV file, initialize SparkContext, and perform basic operations.\n",
    "\n",
    "Tasks:\n",
    "1. Load the movies.csv using SparkContext.textFile.\n",
    "2. Count the total number of movies.\n",
    "3. Filter movies with rating >= 4.\n",
    "4. Display the top 5 movies by rating."
   ],
   "id": "8b945a0ce9adb1df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:25:37.266574Z",
     "start_time": "2025-05-13T17:25:36.629894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load the movies.csv using SparkContext.textFile.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-MovieRatingsAnalytics\")\n",
    "movies_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/5 movies.csv\")"
   ],
   "id": "b59b53435a2dcef9",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Count the total number of movies.\n",
    "total_movies = movies_rdd.count()\n",
    "total_movies"
   ],
   "id": "1a19beca60583a7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1 line = 1 movie\n",
    "\n",
    "Total movies = 5"
   ],
   "id": "e8b6dc235917898e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Filter movies with rating >= 4.\n",
    "movies_rated_4_or_plus = movies_rdd.filter(lambda fields: float(fields[3]) >= 4.0) \\\n",
    "    .map(lambda fields: (fields[1], fields[2], float(fields[3]))) \\\n",
    "    .collect()\n",
    "\n",
    "movies_rated_4_or_plus"
   ],
   "id": "57f2af9179062157",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filter lines with rating >= 4.0 -> We take the second field (movie name), third field (genre) and fourth field (rating).",
   "id": "2a5d027b169c1d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:26:07.070245Z",
     "start_time": "2025-05-13T17:26:07.016567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Display the top 5 movies by rating. (Sort + Limit)\n",
    "top_5_movies = movies_rdd.map(lambda fields: (fields[1], fields[2], float(fields[3]))) \\\n",
    "    .sortBy(lambda fields: float(fields[2]), ascending=False) \\\n",
    "    .take(5)\n",
    "\n",
    "top_5_movies"
   ],
   "id": "4a8383b0550bbf33",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Dark Knight', 'Action', 4.9),\n",
       " ('Inception', 'Sci-Fi', 4.8),\n",
       " ('Interstellar', 'Sci-Fi', 4.7),\n",
       " ('The Notebook', 'Romance', 4.2),\n",
       " ('Fast & Furious', 'Action', 3.9)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second field (movie name), third field (genre) and fourth field (rating) -> Sort by rating -> Take the top 5 movies.",
   "id": "eb694efe0493ce65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:27:55.933458Z",
     "start_time": "2025-05-13T17:27:55.887124Z"
    }
   },
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "ea172b55f8208e90",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 6: Student Scores Analysis\n",
    "Objective: Understand RDD transformations and actions.\n",
    "\n",
    "Tasks:\n",
    "1. Load data using SparkContext.textFile.\n",
    "2. Map each line to a key-value pair (name, score).\n",
    "3. Filter students who scored above 80.\n",
    "4. Count how many students scored above 80."
   ],
   "id": "1f93d1f8e1dbbf0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:27:57.356615Z",
     "start_time": "2025-05-13T17:27:56.763433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load data using SparkContext.textFile.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-StudentScoresAnalysis\")\n",
    "students_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/6 analysis.csv\")\n",
    "\n",
    "students_rdd.collect()"
   ],
   "id": "74e91e69755b597c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Alice', 'Math', '85'],\n",
       " ['Bob', 'Math', '75'],\n",
       " ['Charlie', 'Math', '90'],\n",
       " ['Diana', 'Math', '88'],\n",
       " ['Eve', 'Math', '60']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:29:02.403193Z",
     "start_time": "2025-05-13T17:29:02.383172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Map each line to a key-value pair (name, score).\n",
    "students_scores_rdd = students_rdd.map(lambda fields: (fields[0], float(fields[2])))\n",
    "\n",
    "students_scores_rdd.collect()"
   ],
   "id": "97c4c1ed933ca27f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 85.0),\n",
       " ('Bob', 75.0),\n",
       " ('Charlie', 90.0),\n",
       " ('Diana', 88.0),\n",
       " ('Eve', 60.0)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the first field (name) and the third field (score) -> We create a tuple with the name and score.",
   "id": "4da45129eda5db02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:32:07.150964Z",
     "start_time": "2025-05-13T17:32:07.133540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Filter students who scored above 80.\n",
    "students_above_80 = students_rdd.filter(lambda fields: float(fields[2]) > 80.0) \\\n",
    "    .map(lambda fields: (fields[0], fields[1])) \\\n",
    "    .collect()\n",
    "\n",
    "students_above_80"
   ],
   "id": "f14ae475bdbc9603",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 'Math'), ('Charlie', 'Math'), ('Diana', 'Math')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filter by score > 80.0 (field n째3) -> We take the first field (name) and the second field (subject).",
   "id": "e4507b5f1fb31100"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:33:30.450316Z",
     "start_time": "2025-05-13T17:33:30.398543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Count how many students scored above 80.\n",
    "count_students_above_80 = students_rdd.filter(lambda fields: float(fields[2]) > 80.0) \\\n",
    "    .count()\n",
    "\n",
    "print(count_students_above_80)\n",
    "\n",
    "# or\n",
    "count_students_above_80 = len(students_above_80)\n",
    "count_students_above_80"
   ],
   "id": "f18e3a0cc5bc96ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:35:49.400545Z",
     "start_time": "2025-05-13T17:35:48.404261Z"
    }
   },
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "2fc10282e98e63d5",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 7: Word Count from News Articles\n",
    "Objective: Classic Word Count example.\n",
    "\n",
    "Tasks:\n",
    "1. Load the news data file.\n",
    "2. Split lines into words.\n",
    "3. Map each word to (word, 1).\n",
    "4. Use reduceByKey to get word counts"
   ],
   "id": "66a8a5a8dd32db90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:35:49.465777Z",
     "start_time": "2025-05-13T17:35:49.409574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load the news data file.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-WordCount\")\n",
    "news_rdd = sc.textFile(f\"{DIR_CONTEXT}/7 wordcount.csv\", 0)\n",
    "\n",
    "news_rdd.collect()"
   ],
   "id": "496eef8739fb997c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark is fast. Big data is booming. Spark handles big data with ease.']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:40:33.977180Z",
     "start_time": "2025-05-13T17:40:33.961460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Split lines into words.\n",
    "words_rdd = news_rdd.flatMap(lambda line: line.replace('.', '').split(\" \"))\n",
    "\n",
    "words_rdd.collect()"
   ],
   "id": "2cbf08c569c0397e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark',\n",
       " 'is',\n",
       " 'fast',\n",
       " 'Big',\n",
       " 'data',\n",
       " 'is',\n",
       " 'booming',\n",
       " 'Spark',\n",
       " 'handles',\n",
       " 'big',\n",
       " 'data',\n",
       " 'with',\n",
       " 'ease']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split words by space",
   "id": "cdc33ad32b45bd1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:41:26.243026Z",
     "start_time": "2025-05-13T17:41:26.225904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Map each word to (word, 1).\n",
    "word_pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "word_pairs_rdd.collect()"
   ],
   "id": "1fa449f4aa1ce6ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 1),\n",
       " ('is', 1),\n",
       " ('fast', 1),\n",
       " ('Big', 1),\n",
       " ('data', 1),\n",
       " ('is', 1),\n",
       " ('booming', 1),\n",
       " ('Spark', 1),\n",
       " ('handles', 1),\n",
       " ('big', 1),\n",
       " ('data', 1),\n",
       " ('with', 1),\n",
       " ('ease', 1)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take each word and create a tuple with the word and 1 (1 occurrence).",
   "id": "cacf4113b7408355"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:41:41.599660Z",
     "start_time": "2025-05-13T17:41:41.552616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Use reduceByKey to get word counts\n",
    "word_counts_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "word_counts_rdd.collect()"
   ],
   "id": "135d09cc70022c42",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 2),\n",
       " ('is', 2),\n",
       " ('fast', 1),\n",
       " ('Big', 1),\n",
       " ('data', 2),\n",
       " ('booming', 1),\n",
       " ('handles', 1),\n",
       " ('big', 1),\n",
       " ('with', 1),\n",
       " ('ease', 1)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:43:04.877980Z",
     "start_time": "2025-05-13T17:43:04.532446Z"
    }
   },
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "1c882534758ae3e2",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 8: Product Sales Tracker\n",
    "Objective: Group and aggregate product sales.\n",
    "\n",
    "Tasks:\n",
    "1. Load product sales CSV.\n",
    "2. Map to (product, revenue).\n",
    "3. Use reduceByKey to get total revenue.\n",
    "4. Filter products with revenue > 1000."
   ],
   "id": "63abf9cbdf5e8b49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:43:06.423948Z",
     "start_time": "2025-05-13T17:43:05.827021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load product sales CSV.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-ProductSalesTracker\")\n",
    "product_sales_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/8 product.csv\")\n",
    "\n",
    "product_sales_rdd.collect()"
   ],
   "id": "acd2bde054f8f86d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 'Mobile', '100', '10'],\n",
       " ['2', 'Laptop', '10', '900'],\n",
       " ['3', 'Mouse', '200', '5'],\n",
       " ['4', 'Keyboard', '150', '10']]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:45:25.554118Z",
     "start_time": "2025-05-13T17:45:25.536225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Map to (product, revenue).\n",
    "product_revenue_rdd = product_sales_rdd.map(lambda fields: (fields[1], float(fields[2]) * float(fields[3])))\n",
    "\n",
    "product_revenue_rdd.collect()"
   ],
   "id": "1a617b34711c8ea4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mobile', 1000.0),\n",
       " ('Laptop', 9000.0),\n",
       " ('Mouse', 1000.0),\n",
       " ('Keyboard', 1500.0)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the second field (product) and the third field (revenue) -> We create a tuple with the product and revenue.",
   "id": "b8eceb1e6dcaaf24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:48:54.730536Z",
     "start_time": "2025-05-13T17:48:54.647858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Use reduceByKey to get total revenue.\n",
    "total_revenue_rdd = product_revenue_rdd.reduceByKey(lambda a, b: a + b) # ??\n",
    "\n",
    "total_revenue_rdd.collect()\n",
    "\n",
    "# Getwhole total revenue\n",
    "total_revenue = total_revenue_rdd.map(lambda fields: fields[1]).sum()\n",
    "\n",
    "total_revenue"
   ],
   "id": "b3fcbaf21c5b302f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500.0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this case, the usage of reduceByKey is not necessary since there's no duplicate keys.\n",
    "\n",
    "To get the total revenue, we can simply sum values."
   ],
   "id": "ba4482d83dad47cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:50:43.581539Z",
     "start_time": "2025-05-13T17:50:43.532335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Filter products with revenue > 1000.\n",
    "filtered_revenue_rdd = total_revenue_rdd.filter(lambda fields: fields[1] > 1000)\n",
    "\n",
    "filtered_revenue_rdd.collect()"
   ],
   "id": "ff1bbab7cf093a59",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Laptop', 9000.0), ('Keyboard', 1500.0)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filter the products with revenue > 1000.",
   "id": "d4cdd57e171687eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:51:33.752778Z",
     "start_time": "2025-05-13T17:51:33.409851Z"
    }
   },
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "474edd3c46b285b2",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 9: Temperature Data Monitoring\n",
    "Objective: Work with sensor data and aggregate.\n",
    "\n",
    "Tasks:\n",
    "1. Load temperature log.\n",
    "2. Extract date and temperature.\n",
    "3. Calculate daily average temperature.\n",
    "4. Filter days with average temperature > 30째C."
   ],
   "id": "1c0dd5caec92476b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:51:38.650145Z",
     "start_time": "2025-05-13T17:51:38.048908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load temperature log.\n",
    "sc = SparkContext(\"local\", \"ST2CBD-Lab2-TemperatureDataMonitoring\")\n",
    "temperature_rdd = parse_rdd_csv(f\"{DIR_CONTEXT}/9 temp.csv\")\n",
    "\n",
    "temperature_rdd.collect()"
   ],
   "id": "b0b15eb7f52ecc5f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2025-05-01', 'S1', '32'],\n",
       " ['2025-05-01', 'S2', '31'],\n",
       " ['2025-05-02', 'S1', '28'],\n",
       " ['2025-05-02',\n",
       "  'S2',\n",
       "  '30                                                     ']]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:52:47.761732Z",
     "start_time": "2025-05-13T17:52:47.743159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Extract date and temperature.\n",
    "temperature_data_rdd = temperature_rdd.map(lambda fields: (fields[0], float(fields[2])))\n",
    "\n",
    "temperature_data_rdd.collect()"
   ],
   "id": "38d67bc39ed97bb5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2025-05-01', 32.0),\n",
       " ('2025-05-01', 31.0),\n",
       " ('2025-05-02', 28.0),\n",
       " ('2025-05-02', 30.0)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the first field (date) and the third field (temperature) -> We create a tuple with the date and temperature.",
   "id": "917466b15e65e03f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T17:52:49.074774Z",
     "start_time": "2025-05-13T17:52:49.043451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Calculate daily average temperature.\n",
    "daily_avg_temp_rdd = temperature_data_rdd.groupByKey() \\\n",
    "    .mapValues(lambda temps: sum(temps) / len(temps))\n",
    "\n",
    "daily_avg_temp_rdd.collect()"
   ],
   "id": "28e71c83f959aa54",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2025-05-01', 31.5), ('2025-05-02', 29.0)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We use groupByKey to group by date -> We calculate the average temperature for each date.",
   "id": "9b0562454a77f488"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. Filter days with average temperature > 30째C.\n",
    "filtered_temp_rdd = daily_avg_temp_rdd.filter(lambda fields: fields[1] > 30)\n",
    "\n",
    "filtered_temp_rdd.collect()"
   ],
   "id": "b832daf566de7bc5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We take the average temperature (produced previously) and filter the days with average temperature > 30째C.",
   "id": "8514fa2d386f0029"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sc.stop()",
   "id": "3b6be83e72c52e48"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
